{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5240a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from socket import if_indextoname\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import argparse\n",
    "\n",
    "from env import Box, get_last_states\n",
    "from sac_model import CirclePB, Uniform\n",
    "from sac_sampling import (\n",
    "    sample_trajectories,\n",
    "    evaluate_backward_logprobs,\n",
    ")\n",
    "from sac import SAC\n",
    "from sac_replay_memory import ReplayMemory, trajectories_to_transitions\n",
    "\n",
    "from utils import (\n",
    "    fit_kde,\n",
    "    plot_reward,\n",
    "    sample_from_reward,\n",
    "    plot_samples,\n",
    "    estimate_jsd,\n",
    "    plot_trajectories,\n",
    "    plot_termination_probabilities,\n",
    ")\n",
    "\n",
    "import config\n",
    "import sac_config\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--device\", type=str, default=sac_config.DEVICE)\n",
    "parser.add_argument(\"--dim\", type=int, default=config.DIM)\n",
    "parser.add_argument(\"--delta\", type=float, default=config.DELTA)\n",
    "parser.add_argument(\"--epsilon\", type=float, default=config.EPSILON)\n",
    "parser.add_argument(\"--R0\", type=float, default=config.R0, help=\"Baseline reward value\")\n",
    "parser.add_argument(\"--R1\", type=float, default=config.R1, help=\"Medium reward value (e.g., outer square)\")\n",
    "parser.add_argument(\"--R2\", type=float, default=config.R2, help=\"High reward value (e.g., inner square)\")\n",
    "parser.add_argument(\"--reward_debug\", action=\"store_true\", default=config.REWARD_DEBUG)\n",
    "parser.add_argument(\n",
    "    \"--reward_type\",\n",
    "    type=str,\n",
    "    choices=[\"baseline\", \"ring\", \"angular_ring\", \"multi_ring\", \"curve\", \"gaussian_mixture\"],\n",
    "    default=config.REWARD_TYPE,\n",
    "    help=\"Type of reward function to use. To modify reward-specific parameters (radius, sigma, etc.), edit rewards.py\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta_min\",\n",
    "    type=float,\n",
    "    default=config.BETA_MIN,\n",
    "    help=\"Minimum value for the concentration parameters of the Beta distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta_max\",\n",
    "    type=float, \n",
    "    default=config.BETA_MAX,\n",
    "    help=\"Maximum value for the concentration parameters of the Beta distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PB\",\n",
    "    type=str,\n",
    "    choices=[\"learnable\", \"tied\", \"uniform\"],\n",
    "    default=config.PB,\n",
    "    help=\"Backward policy type\",\n",
    ")\n",
    "parser.add_argument(\"--gamma_scheduler\", type=float, default=config.GAMMA_SCHEDULER)\n",
    "parser.add_argument(\"--scheduler_milestone\", type=int, default=config.SCHEDULER_MILESTONE)\n",
    "parser.add_argument(\"--seed\", type=int, default=config.SEED)\n",
    "parser.add_argument(\"--lr\", type=float, default=config.LR, help=\"Learning rate for SAC\")\n",
    "parser.add_argument(\"--BS\", type=int, default=config.BS)\n",
    "parser.add_argument(\"--n_iterations\", type=int, default=config.N_ITERATIONS)\n",
    "parser.add_argument(\"--n_evaluation_interval\", type=int, default=config.N_EVALUATION_INTERVAL)\n",
    "parser.add_argument(\"--n_logging_interval\", type=int, default=config.N_LOGGING_INTERVAL)\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=config.HIDDEN_DIM)\n",
    "parser.add_argument(\"--n_hidden\", type=int, default=config.N_HIDDEN)\n",
    "parser.add_argument(\"--n_evaluation_trajectories\", type=int, default=config.N_EVALUATION_TRAJECTORIES)\n",
    "parser.add_argument(\"--no_plot\", action=\"store_true\", default=config.NO_PLOT)\n",
    "parser.add_argument(\"--no_wandb\", action=\"store_true\", default=config.NO_WANDB)\n",
    "parser.add_argument(\"--wandb_project\", type=str, default=config.WANDB_PROJECT)\n",
    "parser.add_argument(\"--uniform_ratio\", type=float, default=config.UNIFORM_RATIO, help=\"Ratio of uniform policy\")\n",
    "\n",
    "\n",
    "# SAC-specific arguments\n",
    "parser.add_argument(\"--tau\", type=float, default=sac_config.TAU, help=\"Tau for soft update\")\n",
    "parser.add_argument(\"--target_update_interval\", type=int, default=sac_config.TARGET_UPDATE_INTERVAL, help=\"Target network update interval\")\n",
    "parser.add_argument(\"--Critic_hidden_size\", type=int, default=sac_config.CRITIC_HIDDEN_SIZE, help=\"Hidden size for SAC critic networks\")\n",
    "parser.add_argument(\"--replay_size\", type=int, default=sac_config.REPLAY_SIZE, help=\"Replay buffer size\")\n",
    "parser.add_argument(\"--sac_batch_size\", type=int, default=sac_config.SAC_BATCH_SIZE, help=\"SAC batch size\")\n",
    "parser.add_argument(\"--updates_per_step\", type=int, default=sac_config.UPDATES_PER_STEP, help=\"SAC updates per step\")\n",
    "parser.add_argument(\"--without_backward_model\", type=bool, default=sac_config.WITHOUT_BACKWARD_MODEL, help=\"Whether to use backward model\")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "device = args.device\n",
    "dim = args.dim\n",
    "delta = args.delta\n",
    "epsilon = args.epsilon\n",
    "seed = args.seed\n",
    "lr = args.lr\n",
    "n_iterations = args.n_iterations\n",
    "BS = args.BS\n",
    "\n",
    "if seed == 0:\n",
    "    seed = np.random.randint(int(1e6))\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e7c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def trajectories_to_transitions(trajectories, actionss, all_bw_logprobs, logrewards, env):\n",
    "    \"\"\"\n",
    "    Convert trajectories to transitions for replay buffer.\n",
    "\n",
    "    Args:\n",
    "        trajectories: tensor of shape (batch_size, trajectory_length, dim)\n",
    "        actionss: tensor of shape (batch_size, trajectory_length, dim)\n",
    "        all_bw_logprobs: tensor of shape (batch_size, trajectory_length)\n",
    "        last_states: tensor of shape (batch_size, dim)\n",
    "        logrewards: tensor of shape (batch_size,)\n",
    "        env: environment object\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (states, actions, rewards, next_states, dones) as tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract states and next_states for intermediate transitions\n",
    "    # Match the length to all_bw_logprobs\n",
    "\n",
    "\n",
    "    # Extract states and next_states for intermediate transitions\n",
    "    # Match the length to all_bw_logprobs\n",
    "\n",
    "    states = trajectories[:, :-1, :]  \n",
    "    next_states = trajectories[:, 1:, :] \n",
    "    is_not_sink = torch.all(states != env.sink_state, dim=-1)\n",
    "    is_next_sink = torch.all(next_states == env.sink_state, dim=-1)\n",
    "    last_state = is_not_sink & is_next_sink\n",
    "    dones = torch.zeros_like(last_state, dtype=torch.float32)  # (batch_size, bw_length)\n",
    "    dones[last_state] = 1.0\n",
    "    rewards = torch.cat([all_bw_logprobs, torch.full((all_bw_logprobs.shape[0], 1), float('-inf'), device=all_bw_logprobs.device)], dim=1)\n",
    "    rewards[dones == 1] = logrewards\n",
    "\n",
    "    # Check which rewards are valid (not inf/nan)\n",
    "    is_valid = torch.isfinite(rewards)  # (batch_size, bw_length)\n",
    "\n",
    "    # Flatten batch and time dimensions for transitions\n",
    "    states_flat = states[is_valid]\n",
    "    actions_flat = actionss[is_valid]\n",
    "    rewards_flat = rewards[is_valid]\n",
    "    next_states_flat = next_states[is_valid]\n",
    "    dones_flat = dones[is_valid]\n",
    "\n",
    "    return states_flat, actions_flat, rewards_flat, next_states_flat, dones_flat\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, seed, device='cpu'):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Will be initialized on first push\n",
    "        self.states = None\n",
    "        self.actions = None\n",
    "        self.rewards = None\n",
    "        self.next_states = None\n",
    "        self.dones = None\n",
    "\n",
    "    def push_batch(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Push a batch of transitions to the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            states: tensor of shape (batch_size, state_dim)\n",
    "            actions: tensor of shape (batch_size, action_dim)\n",
    "            rewards: tensor of shape (batch_size,)\n",
    "            next_states: tensor of shape (batch_size, state_dim)\n",
    "            dones: tensor of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "\n",
    "        # Initialize buffers on first push\n",
    "        if self.states is None:\n",
    "            state_dim = states.shape[1]\n",
    "            action_dim = actions.shape[1]\n",
    "            self.states = torch.zeros((self.capacity, state_dim), dtype=states.dtype, device=self.device)\n",
    "            self.actions = torch.zeros((self.capacity, action_dim), dtype=actions.dtype, device=self.device)\n",
    "            self.rewards = torch.zeros(self.capacity, dtype=rewards.dtype, device=self.device)\n",
    "            self.next_states = torch.zeros((self.capacity, state_dim), dtype=next_states.dtype, device=self.device)\n",
    "            self.dones = torch.zeros(self.capacity, dtype=dones.dtype, device=self.device)\n",
    "\n",
    "        # Move to device if needed\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "\n",
    "        # Calculate indices\n",
    "        end_pos = self.position + batch_size\n",
    "\n",
    "        if end_pos <= self.capacity:\n",
    "            # No wrap around\n",
    "            self.states[self.position:end_pos] = states\n",
    "            self.actions[self.position:end_pos] = actions\n",
    "            self.rewards[self.position:end_pos] = rewards\n",
    "            self.next_states[self.position:end_pos] = next_states\n",
    "            self.dones[self.position:end_pos] = dones\n",
    "        else:\n",
    "            # Wrap around\n",
    "            first_part = self.capacity - self.position\n",
    "            self.states[self.position:] = states[:first_part]\n",
    "            self.actions[self.position:] = actions[:first_part]\n",
    "            self.rewards[self.position:] = rewards[:first_part]\n",
    "            self.next_states[self.position:] = next_states[:first_part]\n",
    "            self.dones[self.position:] = dones[:first_part]\n",
    "\n",
    "            second_part = batch_size - first_part\n",
    "            self.states[:second_part] = states[first_part:]\n",
    "            self.actions[:second_part] = actions[first_part:]\n",
    "            self.rewards[:second_part] = rewards[first_part:]\n",
    "            self.next_states[:second_part] = next_states[first_part:]\n",
    "            self.dones[:second_part] = dones[first_part:]\n",
    "\n",
    "        self.position = end_pos % self.capacity\n",
    "        self.size = min(self.size + batch_size, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a random batch from the buffer and return as numpy arrays for compatibility.\"\"\"\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        indices = torch.from_numpy(indices).to(self.device)\n",
    "\n",
    "        return (\n",
    "            self.states[indices],\n",
    "            self.actions[indices],\n",
    "            self.rewards[indices].unsqueeze(-1),  # (batch_size,) -> (batch_size, 1)\n",
    "            self.next_states[indices],\n",
    "            self.dones[indices].unsqueeze(-1),  # (batch_size,) -> (batch_size, 1)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def save_buffer(self, env_name, suffix=\"\", save_path=None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = \"checkpoints/sac_buffer_{}_{}\".format(env_name, suffix)\n",
    "        print('Saving buffer to {}'.format(save_path))\n",
    "\n",
    "        buffer_dict = {\n",
    "            'states': self.states[:self.size].cpu() if self.states is not None else None,\n",
    "            'actions': self.actions[:self.size].cpu() if self.actions is not None else None,\n",
    "            'rewards': self.rewards[:self.size].cpu() if self.rewards is not None else None,\n",
    "            'next_states': self.next_states[:self.size].cpu() if self.next_states is not None else None,\n",
    "            'dones': self.dones[:self.size].cpu() if self.dones is not None else None,\n",
    "            'position': self.position,\n",
    "            'size': self.size,\n",
    "        }\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(buffer_dict, f)\n",
    "\n",
    "    def load_buffer(self, save_path):\n",
    "        print('Loading buffer from {}'.format(save_path))\n",
    "\n",
    "        with open(save_path, \"rb\") as f:\n",
    "            buffer_dict = pickle.load(f)\n",
    "\n",
    "        self.position = buffer_dict['position']\n",
    "        self.size = buffer_dict['size']\n",
    "\n",
    "        if buffer_dict['states'] is not None:\n",
    "            state_dim = buffer_dict['states'].shape[1]\n",
    "            action_dim = buffer_dict['actions'].shape[1]\n",
    "\n",
    "            self.states = torch.zeros((self.capacity, state_dim), device=self.device)\n",
    "            self.actions = torch.zeros((self.capacity, action_dim), device=self.device)\n",
    "            self.rewards = torch.zeros(self.capacity, device=self.device)\n",
    "            self.next_states = torch.zeros((self.capacity, state_dim), device=self.device)\n",
    "            self.dones = torch.zeros(self.capacity, device=self.device)\n",
    "\n",
    "            self.states[:self.size] = buffer_dict['states'].to(self.device)\n",
    "            self.actions[:self.size] = buffer_dict['actions'].to(self.device)\n",
    "            self.rewards[:self.size] = buffer_dict['rewards'].to(self.device)\n",
    "            self.next_states[:self.size] = buffer_dict['next_states'].to(self.device)\n",
    "            self.dones[:self.size] = buffer_dict['dones'].to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b358a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Distribution, Beta\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sample_actions(env, model, states):\n",
    "    # states is a tensor of shape (n, dim)\n",
    "    batch_size = states.shape[0]\n",
    "    out = model.to_dist(states)\n",
    "    if isinstance(out[0], Distribution):  # s0 input\n",
    "        dist_r, dist_theta = out\n",
    "        samples_r = dist_r.rsample(torch.Size((batch_size,)))\n",
    "        samples_theta = dist_theta.rsample(torch.Size((batch_size,)))\n",
    "\n",
    "        actions = (\n",
    "            torch.stack(\n",
    "                [\n",
    "                    samples_r * torch.cos(torch.pi / 2.0 * samples_theta),\n",
    "                    samples_r * torch.sin(torch.pi / 2.0 * samples_theta),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            * env.delta\n",
    "        )\n",
    "\n",
    "        logprobs = (\n",
    "            dist_r.log_prob(samples_r)\n",
    "            + dist_theta.log_prob(samples_theta)\n",
    "            - torch.log(samples_r * env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - np.log(env.delta)  # why ?\n",
    "        )\n",
    "\n",
    "        exit_proba_naive = torch.zeros((batch_size,),device=env.device)\n",
    "        actions_naive = actions\n",
    "        logprobs_naive = logprobs\n",
    "\n",
    "    else:\n",
    "        exit_proba, dist = out\n",
    "        exit_proba_naive = exit_proba.clone()\n",
    "        exit = torch.bernoulli(exit_proba).bool()\n",
    "        near_goal_mask = torch.norm(1 - states, dim=1) <= env.delta\n",
    "        boundary_mask  = torch.any(states >= 1 - env.epsilon, dim=-1)\n",
    "        exit = near_goal_mask | boundary_mask\n",
    "        A = torch.where(\n",
    "            states[:, 0] <= 1 - env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((1 - states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            states[:, 1] <= 1 - env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((1 - states[:, 1]) / env.delta),\n",
    "        )\n",
    "        assert torch.all(\n",
    "            B[~torch.any(states >= 1 - env.delta, dim=-1)]\n",
    "            >= A[~torch.any(states >= 1 - env.delta, dim=-1)]\n",
    "        )\n",
    "        samples = dist.rsample()\n",
    "\n",
    "        actions = samples * (B - A) + A\n",
    "        actions *= torch.pi / 2.0\n",
    "        actions = (\n",
    "            torch.stack([torch.cos(actions), torch.sin(actions)], dim=1) * env.delta\n",
    "        )\n",
    "\n",
    "        actions_naive = actions.clone()\n",
    "        \n",
    "        logprobs = (\n",
    "            dist.log_prob(samples)\n",
    "            + torch.log(1 - exit_proba)\n",
    "            - np.log(env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - torch.log(B - A)\n",
    "        )\n",
    "\n",
    "        logprobs_naive = logprobs.clone()\n",
    "\n",
    "        actions[exit] = -float(\"inf\")\n",
    "        logprobs[exit] = torch.log(exit_proba[exit])\n",
    "        logprobs[near_goal_mask] = 0.0\n",
    "        logprobs[boundary_mask] = 0.0\n",
    "\n",
    "        exit_proba_naive[near_goal_mask] = 1.0\n",
    "        exit_proba_naive[boundary_mask] = 1.0\n",
    "        actions_naive[near_goal_mask] = -float(\"inf\")\n",
    "        actions_naive[boundary_mask] = -float(\"inf\")\n",
    "        logprobs_naive[near_goal_mask] = 0.0\n",
    "        logprobs_naive[boundary_mask] = 0.0\n",
    "\n",
    "    return actions, logprobs, exit_proba_naive, actions_naive, logprobs_naive\n",
    "\n",
    "\n",
    "def sample_trajectories(env, model, n_trajectories):\n",
    "    step = 0\n",
    "    states = torch.zeros((n_trajectories, env.dim), device=env.device)\n",
    "    actionss = []\n",
    "    trajectories = [states]\n",
    "    trajectories_logprobs = torch.zeros((n_trajectories,), device=env.device)\n",
    "    all_logprobs = []\n",
    "    while not torch.all(states == env.sink_state):\n",
    "        step_logprobs = torch.full((n_trajectories,), -float(\"inf\"), device=env.device)\n",
    "        non_terminal_mask = torch.all(states != env.sink_state, dim=-1)\n",
    "        actions = torch.full(\n",
    "            (n_trajectories, env.dim), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        non_terminal_actions, logprobs, _, _, _ = sample_actions(\n",
    "            env,\n",
    "            model,\n",
    "            states[non_terminal_mask],\n",
    "        )\n",
    "        actions[non_terminal_mask] = non_terminal_actions.reshape(-1, env.dim)\n",
    "        actionss.append(actions)\n",
    "        states = env.step(states, actions)\n",
    "        trajectories.append(states)\n",
    "        trajectories_logprobs[non_terminal_mask] += logprobs\n",
    "        step_logprobs[non_terminal_mask] = logprobs\n",
    "        all_logprobs.append(step_logprobs)\n",
    "        step += 1\n",
    "    trajectories = torch.stack(trajectories, dim=1)\n",
    "    actionss = torch.stack(actionss, dim=1)\n",
    "    all_logprobs = torch.stack(all_logprobs, dim=1)\n",
    "    return trajectories, actionss, trajectories_logprobs, all_logprobs\n",
    "\n",
    "\n",
    "def evaluate_backward_logprobs(env, model, trajectories):\n",
    "    logprobs = torch.zeros((trajectories.shape[0],), device=env.device)\n",
    "    all_logprobs = []\n",
    "    for i in range(trajectories.shape[1] - 2, 1, -1):\n",
    "        all_step_logprobs = torch.full(\n",
    "            (trajectories.shape[0],), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        non_sink_mask = torch.all(trajectories[:, i] != env.sink_state, dim=-1)\n",
    "        current_states = trajectories[:, i][non_sink_mask]\n",
    "        previous_states = trajectories[:, i - 1][non_sink_mask]\n",
    "        difference_1 = current_states[:, 0] - previous_states[:, 0]\n",
    "        difference_1.clamp_(\n",
    "            min=0.0, max=env.delta\n",
    "        )  # Should be the case already - just to avoid numerical issues\n",
    "        A = torch.where(\n",
    "            current_states[:, 0] >= env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((current_states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            current_states[:, 1] >= env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((current_states[:, 1]) / env.delta),\n",
    "        )\n",
    "\n",
    "        dist = model.to_dist(current_states)\n",
    "\n",
    "        step_logprobs = (\n",
    "            dist.log_prob(\n",
    "                (\n",
    "                    1.0\n",
    "                    / (B - A)\n",
    "                    * (2.0 / torch.pi * torch.acos(difference_1 / env.delta) - A)\n",
    "                ).clamp(1e-4, 1 - 1e-4)\n",
    "            ).clamp_max(100)\n",
    "            - np.log(env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - torch.log(B - A)\n",
    "        )\n",
    "\n",
    "        if torch.any(torch.isnan(step_logprobs)):\n",
    "            raise ValueError(\"NaN in backward logprobs\")\n",
    "\n",
    "        if torch.any(torch.isinf(step_logprobs)):\n",
    "            raise ValueError(\"Inf in backward logprobs\")\n",
    "\n",
    "        logprobs[non_sink_mask] += step_logprobs\n",
    "        all_step_logprobs[non_sink_mask] = step_logprobs\n",
    "\n",
    "        all_logprobs.append(all_step_logprobs)\n",
    "\n",
    "    all_logprobs.append(torch.zeros((trajectories.shape[0],), device=env.device))\n",
    "    all_logprobs = torch.stack(all_logprobs, dim=1)\n",
    "\n",
    "    return logprobs, all_logprobs.flip(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf6480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta\n",
    "from torch.optim import Adam\n",
    "from sac_utils import soft_update, hard_update\n",
    "from sac_model import QNetwork\n",
    "from sac_model import CirclePF\n",
    "# Initialize Policy weights\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "        \n",
    "        x1 = F.relu(self.linear1(xu))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "\n",
    "        x2 = F.relu(self.linear4(xu))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, dim=2, hidden_dim=64, n_hidden=2, torso=None, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.output_dim = output_dim\n",
    "        if torso is not None:\n",
    "            self.torso = torso\n",
    "        else:\n",
    "            self.torso = nn.Sequential(\n",
    "                nn.Linear(dim, hidden_dim),\n",
    "                nn.ELU(),\n",
    "                *[\n",
    "                    nn.Sequential(\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.ELU(),\n",
    "                    )\n",
    "                    for _ in range(n_hidden)\n",
    "                ],\n",
    "            )\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.output_layer(self.torso(x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class CirclePF(NeuralNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim=64,\n",
    "        n_hidden=2,\n",
    "        beta_min=0.1,\n",
    "        beta_max=2.0,\n",
    "    ):\n",
    "        output_dim = 3 # Only alpha and beta for single Beta distribution\n",
    "        super().__init__(\n",
    "            dim=2, hidden_dim=hidden_dim, n_hidden=n_hidden, output_dim=output_dim\n",
    "        )\n",
    "\n",
    "        # The following parameters are for PF(. | s0)\n",
    "        self.PFs0 = nn.ParameterDict(\n",
    "            {\n",
    "                \"log_alpha_r\": nn.Parameter(torch.zeros(1)),\n",
    "                \"log_alpha_theta\": nn.Parameter(torch.zeros(1)),\n",
    "                \"log_beta_r\": nn.Parameter(torch.zeros(1)),\n",
    "                \"log_beta_theta\": nn.Parameter(torch.zeros(1)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = super().forward(x)\n",
    "        pre_sigmoid_exit = out[..., 0]\n",
    "        log_alpha = out[..., 1]\n",
    "        log_beta = out[..., 2]\n",
    "\n",
    "        exit_proba = torch.sigmoid(pre_sigmoid_exit)\n",
    "        return (\n",
    "            exit_proba,\n",
    "            self.beta_max * torch.sigmoid(log_alpha) + self.beta_min,\n",
    "            self.beta_max * torch.sigmoid(log_beta) + self.beta_min,\n",
    "        )\n",
    "\n",
    "    def to_dist(self, x):\n",
    "        if torch.all(x[0] == 0.0):\n",
    "            assert torch.all(\n",
    "                x == 0.0\n",
    "            )  # If one of the states is s0, all of them must be\n",
    "            alpha_r = self.PFs0[\"log_alpha_r\"]\n",
    "            alpha_r = self.beta_max * torch.sigmoid(alpha_r) + self.beta_min\n",
    "            alpha_theta = self.PFs0[\"log_alpha_theta\"]\n",
    "            alpha_theta = self.beta_max * torch.sigmoid(alpha_theta) + self.beta_min\n",
    "            beta_r = self.PFs0[\"log_beta_r\"]\n",
    "            beta_r = self.beta_max * torch.sigmoid(beta_r) + self.beta_min\n",
    "            beta_theta = self.PFs0[\"log_beta_theta\"]\n",
    "            beta_theta = self.beta_max * torch.sigmoid(beta_theta) + self.beta_min\n",
    "\n",
    "            dist_r = Beta(alpha_r[0], beta_r[0])\n",
    "            dist_theta = Beta(alpha_theta[0], beta_theta[0])\n",
    "            return dist_r, dist_theta\n",
    "\n",
    "        # Otherwise, we use the neural network\n",
    "        exit_proba, alpha, beta = self.forward(x)\n",
    "        dist = Beta(alpha, beta)\n",
    "\n",
    "        return exit_proba, dist\n",
    "\n",
    "class Uniform():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def to_dist(self, x):\n",
    "        # Set device to match x (input tensor)\n",
    "        device = x.device\n",
    "        if torch.all(x[0] == 0.0):\n",
    "            assert torch.all(\n",
    "                x == 0.0\n",
    "            )  # If one of the states is s0, all of them must be\n",
    "            return Beta(torch.tensor(1., device=device), torch.tensor(1., device=device)), Beta(torch.tensor(1., device=device), torch.tensor(1., device=device))\n",
    "        return Beta(torch.tensor(1., device=device), torch.tensor(1., device=device))\n",
    "\n",
    "class CirclePB(NeuralNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim=64,\n",
    "        n_hidden=2,\n",
    "        torso=None,\n",
    "        uniform=False,\n",
    "        beta_min=0.1,\n",
    "        beta_max=2.0,\n",
    "    ):\n",
    "        output_dim = 2  # Only alpha and beta for single Beta distribution\n",
    "        super().__init__(\n",
    "            dim=2, hidden_dim=hidden_dim, n_hidden=n_hidden, output_dim=output_dim\n",
    "        )\n",
    "        if torso is not None:\n",
    "            self.torso = torso\n",
    "        self.uniform = uniform\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a batch of states, a tensor of shape (batch_size, dim) with dim == 2\n",
    "        out = super().forward(x)\n",
    "        log_alpha = out[:, 0]\n",
    "        log_beta = out[:, 1]\n",
    "        return (\n",
    "            self.beta_max * torch.sigmoid(log_alpha) + self.beta_min,\n",
    "            self.beta_max * torch.sigmoid(log_beta) + self.beta_min,\n",
    "        )\n",
    "\n",
    "    def to_dist(self, x):\n",
    "        if self.uniform:\n",
    "            return Beta(torch.ones(x.shape[0], device=x.device), torch.ones(x.shape[0], device=x.device))\n",
    "        alpha, beta = self.forward(x)\n",
    "        dist = Beta(alpha, beta)\n",
    "        return dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SAC(object):\n",
    "    def __init__(self, args, env):\n",
    "\n",
    "        self.gamma = 1.0  # Fixed to 1.0 for GFlowNet\n",
    "        self.target_update_interval = args.target_update_interval\n",
    "        self.device = env.device\n",
    "        self.tau = args.tau\n",
    "        self.env = env  # Store env for sample_actions\n",
    "        self.critic = QNetwork(env.dim, env.dim, args.Critic_hidden_size).to(device=self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args.lr)\n",
    "        self.critic_target = QNetwork(env.dim, env.dim, args.Critic_hidden_size).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "        self.policy = CirclePF(\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            n_hidden=args.n_hidden,\n",
    "            beta_min=args.beta_min,\n",
    "            beta_max=args.beta_max,\n",
    "        ).to(self.device)\n",
    "        self.policy_optim = Adam(self.policy.parameters(), lr=args.lr)\n",
    "        # Add scheduler for policy optimizer\n",
    "        self.policy_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            self.policy_optim,\n",
    "            milestones=[i * args.scheduler_milestone for i in range(1, 10)],\n",
    "            gamma=args.gamma_scheduler,\n",
    "        )\n",
    "\n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        # Sample a batch from memory\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(batch_size=batch_size)\n",
    "\n",
    "        # ★ 여기에서 그래프를 확실히 끊어주기\n",
    "        state_batch      = state_batch.detach().to(self.device)\n",
    "        action_batch     = action_batch.detach().to(self.device)\n",
    "        reward_batch     = reward_batch.detach().to(self.device)\n",
    "        next_state_batch = next_state_batch.detach().to(self.device)\n",
    "        done_batch       = done_batch.detach().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Separate next_state_batch based on done flag\n",
    "            done_mask = (done_batch == 1).squeeze(-1)  # (batch_size,)\n",
    "            # print(done_mask)\n",
    "            next_state_not_done = next_state_batch[~done_mask]  # States where done == 0\n",
    "            # print(next_state_not_done)\n",
    "            target_q_value = reward_batch[~done_mask].squeeze(-1)\n",
    "            # print(target_q_value)           \n",
    "\n",
    "            _, _, next_state_exit_proba, next_state_action_naive, next_state_log_pi_naive = sample_actions(self.env, self.policy, next_state_not_done)\n",
    "\n",
    "            is_inf_mask = torch.all(torch.isinf(next_state_action_naive), dim=-1)\n",
    "        #     # 1. 반드시 terminal state에 도달하는 경우\n",
    "\n",
    "            target_q_value += next_state_exit_proba * (self.env.reward(next_state_not_done) - next_state_exit_proba.log())\n",
    "        #     # 2. 반드시 terminal state에 도달하지 않아도 되는 경우 \n",
    "\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_not_done[~is_inf_mask], next_state_action_naive[~is_inf_mask])\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target).squeeze(-1) \n",
    "\n",
    "            target_q_value[~is_inf_mask] += (1 - next_state_exit_proba[~is_inf_mask]) * (min_qf_next_target - next_state_log_pi_naive[~is_inf_mask])\n",
    "\n",
    "        qf1, qf2 = self.critic(state_batch[~done_mask], action_batch[~done_mask])  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1 = qf1.squeeze(-1)\n",
    "        qf2 = qf2.squeeze(-1)\n",
    "        qf1_loss = F.mse_loss(qf1, target_q_value)  \n",
    "        qf2_loss = F.mse_loss(qf2, target_q_value)  \n",
    "\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        s0_mask = torch.all(state_batch == 0, dim=-1)\n",
    "        non_s0_mask = ~s0_mask\n",
    "\n",
    "        policy_loss = torch.zeros_like(reward_batch).squeeze(-1)\n",
    "\n",
    "        if s0_mask.any():\n",
    "            _, _, exit_proba_s0, action_naive_s0, log_pi_naive_s0 = sample_actions(self.env, self.policy, state_batch[s0_mask])\n",
    "\n",
    "        if non_s0_mask.any():\n",
    "            _, _, exit_proba_non_s0, action_naive_non_s0, log_pi_naive_non_s0 = sample_actions(self.env, self.policy, state_batch[non_s0_mask])\n",
    "\n",
    "        exit_proba = torch.cat([exit_proba_s0, exit_proba_non_s0], dim=0)\n",
    "        action_naive = torch.cat([action_naive_s0, action_naive_non_s0], dim=0)\n",
    "        log_pi_naive = torch.cat([log_pi_naive_s0, log_pi_naive_non_s0], dim=0)\n",
    "        state_batch_reordered = torch.cat([state_batch[s0_mask], state_batch[non_s0_mask]], dim=0)\n",
    "        is_inf_mask = torch.all(torch.isinf(action_naive), dim=-1)\n",
    "        is_non_zero_mask = torch.where(exit_proba != 0, True, False)\n",
    "\n",
    "        policy_loss[is_non_zero_mask] += exit_proba[is_non_zero_mask] * (exit_proba[is_non_zero_mask].log() - self.env.reward(state_batch_reordered[is_non_zero_mask]))\n",
    "\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch_reordered[~is_inf_mask], action_naive[~is_inf_mask])\n",
    "\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "        min_qf_pi = min_qf_pi.squeeze(-1)\n",
    "        policy_loss[~is_inf_mask] += (1 - exit_proba[~is_inf_mask]) * (log_pi_naive[~is_inf_mask] - min_qf_pi)\n",
    "        policy_loss = policy_loss.mean()\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item()\n",
    "        \n",
    "    # Save model parameters\n",
    "    def save_checkpoint(self, env_name, suffix=\"\", ckpt_path=None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "        if ckpt_path is None:\n",
    "            ckpt_path = \"checkpoints/sac_checkpoint_{}_{}\".format(env_name, suffix)\n",
    "        print('Saving models to {}'.format(ckpt_path))\n",
    "        torch.save({'policy_state_dict': self.policy.state_dict(),\n",
    "                    'critic_state_dict': self.critic.state_dict(),\n",
    "                    'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "                    'critic_optimizer_state_dict': self.critic_optim.state_dict(),\n",
    "                    'policy_optimizer_state_dict': self.policy_optim.state_dict()}, ckpt_path)\n",
    "\n",
    "    # Load model parameters\n",
    "    def load_checkpoint(self, ckpt_path, evaluate=False):\n",
    "        print('Loading models from {}'.format(ckpt_path))\n",
    "        if ckpt_path is not None:\n",
    "            checkpoint = torch.load(ckpt_path)\n",
    "            self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "            self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "            self.critic_optim.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "            self.policy_optim.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
    "\n",
    "            if evaluate:\n",
    "                self.policy.eval()\n",
    "                self.critic.eval()\n",
    "                self.critic_target.eval()\n",
    "            else:\n",
    "                self.policy.train()\n",
    "                self.critic.train()\n",
    "                self.critic_target.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c1b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 63/5000 [00:14<19:12,  4.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(memory) \u001b[38;5;241m>\u001b[39m args\u001b[38;5;241m.\u001b[39msac_batch_size:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mupdates_per_step):\n\u001b[0;32m---> 71\u001b[0m         qf1_loss, qf2_loss, policy_loss \u001b[38;5;241m=\u001b[39m \u001b[43msac_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msac_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msac_updates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m         sac_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Step the scheduler once per iteration (not per update)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 218\u001b[0m, in \u001b[0;36mSAC.update_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, memory, batch_size, updates):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# Sample a batch from memory\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     state_batch, action_batch, reward_batch, next_state_batch, done_batch \u001b[38;5;241m=\u001b[39m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# ★ 여기에서 그래프를 확실히 끊어주기\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     state_batch      \u001b[38;5;241m=\u001b[39m state_batch\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m, in \u001b[0;36mReplayMemory.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sample a random batch from the buffer and return as numpy arrays for compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(indices)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[indices],\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[indices],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdones[indices]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# (batch_size,) -> (batch_size, 1)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Box(\n",
    "    dim=dim,\n",
    "    delta=delta,\n",
    "    epsilon=epsilon,\n",
    "    device_str=device,\n",
    "    reward_type=args.reward_type,\n",
    "    reward_debug=args.reward_debug,\n",
    "    R0=args.R0,\n",
    "    R1=args.R1,\n",
    "    R2=args.R2,\n",
    ")\n",
    "\n",
    "# Create SAC agent (includes CirclePF as policy)\n",
    "sac_agent = SAC(args, env)\n",
    "Uniform_model = Uniform()\n",
    "\n",
    "# Create replay memory\n",
    "memory = ReplayMemory(args.replay_size, seed, device=device)\n",
    "\n",
    "bw_model = CirclePB(\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    n_hidden=args.n_hidden,\n",
    "    torso=sac_agent.policy.torso if args.PB == \"tied\" else None,\n",
    "    uniform=args.PB == \"uniform\",\n",
    "    beta_min=args.beta_min,\n",
    "    beta_max=args.beta_max,\n",
    ").to(device)\n",
    "\n",
    "jsd = float(\"inf\")\n",
    "sac_updates = 0  # Track SAC update steps\n",
    "\n",
    "for i in trange(1, n_iterations + 1):\n",
    "    with torch.no_grad():   # ★ 여기 추가\n",
    "        if np.random.rand() < args.uniform_ratio:\n",
    "            trajectories, actionss, _, _  = sample_trajectories(\n",
    "                env,\n",
    "                Uniform_model,\n",
    "                BS,\n",
    "            )\n",
    "        else:\n",
    "            trajectories, actionss, _, _  = sample_trajectories(\n",
    "                env,\n",
    "                sac_agent.policy,\n",
    "                BS,\n",
    "            )\n",
    "\n",
    "        last_states = get_last_states(env, trajectories)\n",
    "        logrewards = env.reward(last_states).log()\n",
    "        \n",
    "        bw_logprobs, all_bw_logprobs = evaluate_backward_logprobs(\n",
    "            env, bw_model, trajectories\n",
    "        )\n",
    "\n",
    "        if args.without_backward_model:\n",
    "            intermediate_rewards = torch.where(\n",
    "                all_bw_logprobs != -float(\"inf\"),\n",
    "                torch.zeros_like(all_bw_logprobs),\n",
    "                all_bw_logprobs,\n",
    "                )\n",
    "        else:\n",
    "            intermediate_rewards = all_bw_logprobs\n",
    "\n",
    "        # Convert trajectories to transitions and push to replay memory\n",
    "        all_states, all_actions, all_rewards, all_next_states, all_dones = trajectories_to_transitions(\n",
    "            trajectories, actionss, intermediate_rewards, logrewards, env\n",
    "        )\n",
    "        \n",
    "    memory.push_batch(all_states, all_actions, all_rewards, all_next_states, all_dones)\n",
    "    if len(memory) > args.sac_batch_size:\n",
    "        for _ in range(args.updates_per_step):\n",
    "            qf1_loss, qf2_loss, policy_loss = sac_agent.update_parameters(memory, args.sac_batch_size, sac_updates)\n",
    "            sac_updates += 1\n",
    "        # Step the scheduler once per iteration (not per update)\n",
    "        sac_agent.policy_scheduler.step()\n",
    "\n",
    "    if any(\n",
    "        [\n",
    "            torch.isnan(list(sac_agent.policy.parameters())[i]).any()\n",
    "            for i in range(len(list(sac_agent.policy.parameters())))\n",
    "        ]\n",
    "    ):\n",
    "        raise ValueError(\"NaN in model parameters\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a6f396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dbd302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
