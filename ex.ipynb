{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5240a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import argparse\n",
    "\n",
    "from env import Box, get_last_states\n",
    "from model import CirclePF, CirclePB, NeuralNet\n",
    "from sampling import (\n",
    "    sample_trajectories,\n",
    "    evaluate_backward_logprobs,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    fit_kde,\n",
    "    plot_reward,\n",
    "    sample_from_reward,\n",
    "    plot_samples,\n",
    "    estimate_jsd,\n",
    "    plot_trajectories,\n",
    ")\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c1b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--device\", type=str, default=config.DEVICE)\n",
    "parser.add_argument(\"--dim\", type=int, default=config.DIM)\n",
    "parser.add_argument(\"--delta\", type=float, default=config.DELTA)\n",
    "parser.add_argument(\n",
    "    \"--n_components\",\n",
    "    type=int,\n",
    "    default=config.N_COMPONENTS,\n",
    "    help=\"Number of components in Mixture Of Betas\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--reward_debug\", action=\"store_true\", default=config.REWARD_DEBUG)\n",
    "parser.add_argument(\n",
    "    \"--reward_type\",\n",
    "    type=str,\n",
    "    choices=[\"baseline\", \"ring\", \"angular_ring\", \"multi_ring\", \"curve\", \"gaussian_mixture\", \"corner_squares\", \"two_corners\", \"edge_boxes\", \"edge_boxes_corner_squares\"],\n",
    "    default=config.REWARD_TYPE,\n",
    "    help=\"Type of reward function to use. To modify reward-specific parameters (radius, sigma, etc.), edit rewards.py\"\n",
    ")\n",
    "parser.add_argument(\"--R0\", type=float, default=config.R0, help=\"Baseline reward value\")\n",
    "parser.add_argument(\"--R1\", type=float, default=config.R1, help=\"Medium reward value (e.g., outer square)\")\n",
    "parser.add_argument(\"--R2\", type=float, default=config.R2, help=\"High reward value (e.g., inner square)\")\n",
    "parser.add_argument(\n",
    "    \"--n_components_s0\",\n",
    "    type=int,\n",
    "    default=config.N_COMPONENTS_S0,\n",
    "    help=\"Number of components in Mixture Of Betas\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta_min\",\n",
    "    type=float,\n",
    "    default=config.BETA_MIN,\n",
    "    help=\"Minimum value for the concentration parameters of the Beta distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta_max\",\n",
    "    type=float,\n",
    "    default=config.BETA_MAX,\n",
    "    help=\"Maximum value for the concentration parameters of the Beta distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PB\",\n",
    "    type=str,\n",
    "    choices=[\"learnable\", \"tied\", \"uniform\"],\n",
    "    default=config.PB,\n",
    ")\n",
    "parser.add_argument(\"--gamma_scheduler\", type=float, default=config.GAMMA_SCHEDULER)\n",
    "parser.add_argument(\"--scheduler_milestone\", type=int, default=config.SCHEDULER_MILESTONE)\n",
    "parser.add_argument(\"--seed\", type=int, default=config.SEED)\n",
    "parser.add_argument(\"--lr\", type=float, default=config.LR)\n",
    "parser.add_argument(\"--lr_Z\", type=float, default=config.LR_Z)\n",
    "parser.add_argument(\"--lr_F\", type=float, default=config.LR_F)\n",
    "parser.add_argument(\"--tie_F\", action=\"store_true\", default=config.TIE_F)\n",
    "parser.add_argument(\"--BS\", type=int, default=config.BS)\n",
    "parser.add_argument(\"--n_iterations\", type=int, default=config.N_ITERATIONS)\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=config.HIDDEN_DIM)\n",
    "parser.add_argument(\"--n_hidden\", type=int, default=config.N_HIDDEN)\n",
    "parser.add_argument(\"--n_evaluation_trajectories\", type=int, default=config.N_EVALUATION_TRAJECTORIES)\n",
    "parser.add_argument(\"--no_plot\", action=\"store_true\", default=config.NO_PLOT)\n",
    "parser.add_argument(\"--no_wandb\", action=\"store_true\", default=config.NO_WANDB)\n",
    "parser.add_argument(\"--wandb_project\", type=str, default=config.WANDB_PROJECT)\n",
    "\n",
    "# Use parse_args([]) in Jupyter to avoid conflicts with Jupyter's kernel arguments\n",
    "args = parser.parse_args([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229b8a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = args.device\n",
    "dim = args.dim\n",
    "delta = args.delta\n",
    "seed = args.seed\n",
    "lr = args.lr\n",
    "lr_Z = args.lr_Z\n",
    "lr_F = args.lr_F\n",
    "n_iterations = args.n_iterations\n",
    "BS = args.BS\n",
    "n_components = args.n_components\n",
    "n_components_s0 = args.n_components_s0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "env = Box(\n",
    "    dim=dim,\n",
    "    delta=delta,\n",
    "    device_str=device,\n",
    "    reward_type=args.reward_type,\n",
    "    reward_debug=args.reward_debug,\n",
    "    R0=args.R0,\n",
    "    R1=args.R1,\n",
    "    R2=args.R2,\n",
    ")\n",
    "\n",
    "# Get the true KDE\n",
    "samples = sample_from_reward(env, n_samples=10000)\n",
    "true_kde, fig1 = fit_kde(samples, plot=True)\n",
    "\n",
    "\n",
    "model = CirclePF(\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    n_hidden=args.n_hidden,\n",
    "    n_components=n_components,\n",
    "    n_components_s0=n_components_s0,\n",
    "    beta_min=args.beta_min,\n",
    "    beta_max=args.beta_max,\n",
    ").to(device)\n",
    "\n",
    "bw_model = CirclePB(\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    n_hidden=args.n_hidden,\n",
    "    torso=model.torso if args.PB == \"tied\" else None,\n",
    "    uniform=args.PB == \"uniform\",\n",
    "    n_components=n_components,\n",
    "    beta_min=args.beta_min,\n",
    "    beta_max=args.beta_max,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "logZ = torch.zeros(1, requires_grad=True, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "if args.PB != \"uniform\":\n",
    "    optimizer.add_param_group(\n",
    "        {\n",
    "            \"params\": bw_model.output_layer.parameters()\n",
    "            if args.PB == \"tied\"\n",
    "            else bw_model.parameters(),\n",
    "            \"lr\": lr,\n",
    "        }\n",
    "    )\n",
    "optimizer.add_param_group({\"params\": [logZ], \"lr\": lr_Z})\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[i * args.scheduler_milestone for i in range(1, 10)],\n",
    "    gamma=args.gamma_scheduler,\n",
    ")\n",
    "\n",
    "jsd = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b5e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 8, 2])\n",
      "torch.Size([256, 7, 2])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in trange(n_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    trajectories, actionss, logprobs, all_logprobs = sample_trajectories(\n",
    "        env,\n",
    "        model,\n",
    "        BS,\n",
    "    )\n",
    "\n",
    "\n",
    "    last_states = get_last_states(env, trajectories)\n",
    "    logrewards = env.reward(last_states).log()\n",
    "    bw_logprobs, all_bw_logprobs = evaluate_backward_logprobs(\n",
    "        env, bw_model, trajectories\n",
    "    )\n",
    "    print(trajectories.shape)\n",
    "    print(actionss.shape)\n",
    "    print(logprobs.shape)\n",
    "    print(all_logprobs.shape)\n",
    "    break\n",
    "    # TB (Trajectory Balance) loss\n",
    "    loss = torch.mean((logZ + logprobs - bw_logprobs - logrewards) ** 2)\n",
    "\n",
    "    if torch.isinf(loss):\n",
    "        raise ValueError(\"Infinite loss\")\n",
    "    loss.backward()\n",
    "    # clip the gradients for bw_model\n",
    "    for p in bw_model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.data.clamp_(-10, 10).nan_to_num_(0.0)\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.data.clamp_(-10, 10).nan_to_num_(0.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if any(\n",
    "        [\n",
    "            torch.isnan(list(model.parameters())[i]).any()\n",
    "            for i in range(len(list(model.parameters())))\n",
    "        ]\n",
    "    ):\n",
    "        raise ValueError(\"NaN in model parameters\")\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        log_dict = {\n",
    "            \"loss\": loss.item(),\n",
    "            \"sqrt(logZdiff**2)\": np.sqrt((np.log(env.Z) - logZ.item())**2),\n",
    "            \"states_visited\": (i + 1) * BS,\n",
    "        }\n",
    "\n",
    "        # Evaluate JSD every 500 iterations and add to the same log\n",
    "        if i % 500 == 0:\n",
    "            trajectories, _, _, _ = sample_trajectories(\n",
    "                env, model, args.n_evaluation_trajectories\n",
    "            )\n",
    "            last_states = get_last_states(env, trajectories)\n",
    "            kde, fig4 = fit_kde(last_states, plot=True)\n",
    "            jsd = estimate_jsd(kde, true_kde)\n",
    "\n",
    "            log_dict[\"JSD\"] = jsd\n",
    "\n",
    "            if not NO_PLOT:\n",
    "                colors = plt.cm.rainbow(np.linspace(0, 1, 10))\n",
    "                fig1 = plot_samples(last_states[:2000].detach().cpu().numpy())\n",
    "                fig2 = plot_trajectories(trajectories.detach().cpu().numpy()[:20])\n",
    "\n",
    "                log_dict[\"last_states\"] = wandb.Image(fig1)\n",
    "                log_dict[\"trajectories\"] = wandb.Image(fig2)\n",
    "                log_dict[\"kde\"] = wandb.Image(fig4)\n",
    "\n",
    "        if USE_WANDB:\n",
    "            wandb.log(log_dict, step=i)\n",
    "\n",
    "        tqdm.write(\n",
    "            # Loss with 3 digits of precision, logZ with 2 digits of precision, true logZ with 2 digits of precision\n",
    "            # Last computed JSD with 4 digits of precision\n",
    "            f\"States: {(i + 1) * BS}, Loss: {loss.item():.3f}, logZ: {logZ.item():.2f}, true logZ: {np.log(env.Z):.2f}, JSD: {jsd:.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# if USE_WANDB:\n",
    "#     wandb.finish()\n",
    "\n",
    "# # Save model and arguments as JSON\n",
    "# save_path = os.path.join(\"saved_models\", run_name)\n",
    "# if not os.path.exists(save_path):\n",
    "#     os.makedirs(save_path)\n",
    "#     torch.save(model.state_dict(), os.path.join(save_path, \"model.pt\"))\n",
    "#     torch.save(bw_model.state_dict(), os.path.join(save_path, \"bw_model.pt\"))\n",
    "#     torch.save(logZ, os.path.join(save_path, \"logZ.pt\"))\n",
    "#     with open(os.path.join(save_path, \"args.json\"), \"w\") as f:\n",
    "#         json.dump(vars(args), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593944b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4ac6bd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trajectories_to_transitions(trajectories, actionss, all_bw_logprobs, logrewards, env):\n",
    "    \"\"\"\n",
    "    Convert trajectories to transitions for replay buffer.\n",
    "\n",
    "    Args:\n",
    "        trajectories: tensor of shape (batch_size, trajectory_length, dim)\n",
    "        actionss: tensor of shape (batch_size, trajectory_length, dim)\n",
    "        all_bw_logprobs: tensor of shape (batch_size, trajectory_length)\n",
    "        last_states: tensor of shape (batch_size, dim)\n",
    "        logrewards: tensor of shape (batch_size,)\n",
    "        env: environment object\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (states, actions, rewards, next_states, dones) as tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract states and next_states for intermediate transitions\n",
    "    # Match the length to all_bw_logprobs\n",
    "\n",
    "    states = trajectories[:, :-1, :]  \n",
    "    next_states = trajectories[:, 1:, :] \n",
    "    is_not_sink = torch.all(states != env.sink_state, dim=-1)\n",
    "    is_next_sink = torch.all(next_states == env.sink_state, dim=-1)\n",
    "    last_state = is_not_sink & is_next_sink\n",
    "    dones = torch.zeros_like(last_state, dtype=torch.float32)  # (batch_size, bw_length)\n",
    "    dones[last_state] = 1.0\n",
    "    dones = dones[:, 1:]\n",
    "    rewards = all_bw_logprobs\n",
    "    rewards = torch.where(last_state[:,1:], rewards + logrewards.unsqueeze(1), rewards)\n",
    "    states = states[:, :-1, :]  \n",
    "    next_states = next_states[:, :-1, :] \n",
    "    actions = actionss[:, :-1, :] \n",
    "    # Check which rewards are valid (not inf/nan)\n",
    "    is_valid = torch.isfinite(rewards)  # (batch_size, bw_length)\n",
    "\n",
    "    # Flatten batch and time dimensions for transitions\n",
    "    states_flat = states[is_valid]\n",
    "    actions_flat = actions[is_valid]\n",
    "    rewards_flat = rewards[is_valid]\n",
    "    next_states_flat = next_states[is_valid]\n",
    "    dones_flat = dones[is_valid]\n",
    "\n",
    "    return states_flat, actions_flat, rewards_flat, next_states_flat, dones_flat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8ac2157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===0===\n",
      "tensor([0., 0.], device='cuda:0')\n",
      "tensor([0.0515, 0.0231], device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.0515, 0.0231], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===1===\n",
      "tensor([0.0515, 0.0231], device='cuda:0')\n",
      "tensor([0.1983, 0.1522], device='cuda:0')\n",
      "tensor(3.4034, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.2498, 0.1753], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===2===\n",
      "tensor([0.2498, 0.1753], device='cuda:0')\n",
      "tensor([0.1348, 0.2105], device='cuda:0')\n",
      "tensor(3.8133, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.3846, 0.3859], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===3===\n",
      "tensor([0.3846, 0.3859], device='cuda:0')\n",
      "tensor([0.1413, 0.2063], device='cuda:0')\n",
      "tensor(3.9550, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.5259, 0.5921], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===4===\n",
      "tensor([0.5259, 0.5921], device='cuda:0')\n",
      "tensor([0.2142, 0.1288], device='cuda:0')\n",
      "tensor(3.8797, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.7402, 0.7210], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===5===\n",
      "tensor([0.7402, 0.7210], device='cuda:0')\n",
      "tensor([0.1813, 0.1721], device='cuda:0')\n",
      "tensor(218.5743, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.9215, 0.8930], device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "===6===\n",
      "tensor([0., 0.], device='cuda:0')\n",
      "tensor([0.0447, 0.0762], device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.0447, 0.0762], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===7===\n",
      "tensor([0.0447, 0.0762], device='cuda:0')\n",
      "tensor([0.1100, 0.2245], device='cuda:0')\n",
      "tensor(8.5290, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.1547, 0.3007], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===8===\n",
      "tensor([0.1547, 0.3007], device='cuda:0')\n",
      "tensor([0.1569, 0.1946], device='cuda:0')\n",
      "tensor(4.2486, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.3116, 0.4953], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===9===\n",
      "tensor([0.3116, 0.4953], device='cuda:0')\n",
      "tensor([0.1509, 0.1993], device='cuda:0')\n",
      "tensor(4.1450, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.4626, 0.6946], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===10===\n",
      "tensor([0.4626, 0.6946], device='cuda:0')\n",
      "tensor([0.2444, 0.0524], device='cuda:0')\n",
      "tensor(1.4041, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.7070, 0.7471], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===11===\n",
      "tensor([0.7070, 0.7471], device='cuda:0')\n",
      "tensor([0.1234, 0.2174], device='cuda:0')\n",
      "tensor(0.8041, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.8304, 0.9645], device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "===12===\n",
      "tensor([0., 0.], device='cuda:0')\n",
      "tensor([0.0888, 0.0397], device='cuda:0')\n",
      "tensor(1., device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.0888, 0.0397], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===13===\n",
      "tensor([0.0888, 0.0397], device='cuda:0')\n",
      "tensor([0.2014, 0.1481], device='cuda:0')\n",
      "tensor(5.0174, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.2902, 0.1878], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "===14===\n",
      "tensor([0.2902, 0.1878], device='cuda:0')\n",
      "tensor([0.2145, 0.1284], device='cuda:0')\n",
      "tensor(3.8501, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "tensor([0.5047, 0.3162], device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in trange(n_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    trajectories, actionss, logprobs, all_logprobs = sample_trajectories(\n",
    "        env,\n",
    "        model,\n",
    "        BS,\n",
    "    )\n",
    "\n",
    "    last_states = get_last_states(env, trajectories)\n",
    "    logrewards = env.reward(last_states).log()\n",
    "    bw_logprobs, all_bw_logprobs = evaluate_backward_logprobs(\n",
    "        env, bw_model, trajectories\n",
    "    )\n",
    "    all_states, all_actions, all_rewards, all_next_states, all_dones = trajectories_to_transitions(\n",
    "        trajectories, actionss, all_bw_logprobs, logrewards, env\n",
    "    )\n",
    "\n",
    "\n",
    "    for i in range(15):\n",
    "        print(f\"==={i}===\")\n",
    "        print(all_states[i])\n",
    "        print(all_actions[i])\n",
    "        print(torch.exp(all_rewards[i]))\n",
    "        print(all_next_states[i])\n",
    "        print(all_dones[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f43b5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Beta\n",
    "\n",
    "\n",
    "class CirclePF_Uniform():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def to_dist(self, x):\n",
    "        if torch.all(x[0] == 0.0):\n",
    "            assert torch.all(\n",
    "                x == 0.0\n",
    "            )  # If one of the states is s0, all of them must be\n",
    "            alpha = torch.ones(x.shape[0], device=x.device)\n",
    "            beta = torch.ones(x.shape[0], device=x.device)\n",
    "            \n",
    "            return Beta(alpha, beta), Beta(alpha, beta)\n",
    "        \n",
    "        alpha = torch.ones(x.shape[0], device=x.device)\n",
    "        beta = torch.ones(x.shape[0], device=x.device)\n",
    "        return Beta(alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c92cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_ex = torch.rand(10, 2, device=env.device)\n",
    "states_ex_0 = torch.zeros(10, 2, device=env.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01ba9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ations, logprobs, samples = sample_actions(env, model, states_ex)\n",
    "ations_0, logprobs_0, samples_r_0, samples_theta_0 = sample_actions(env, model, states_ex_0)\n",
    "samples_0 = torch.cat([samples_r_0.unsqueeze(-1), samples_theta_0.unsqueeze(-1)], dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bfa7894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "print(ations.shape)\n",
    "print(samples.shape)\n",
    "print(samples_r_0.shape)\n",
    "print(samples_theta_0.shape)\n",
    "print(samples_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db54b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_actions(env, model, states):\n",
    "    # states is a tensor of shape (n, dim)\n",
    "    batch_size = states.shape[0]\n",
    "    out = model.to_dist(states)\n",
    "    if isinstance(out, tuple):  # s0 input returns (dist_r, dist_theta)\n",
    "        dist_r, dist_theta = out\n",
    "        if model.uniform_ratio:\n",
    "            samples_r = torch.rand(batch_size, device=env.device)\n",
    "            samples_theta = torch.rand(batch_size, device=env.device)\n",
    "        else:\n",
    "            samples_r = dist_r.sample(torch.Size((batch_size,)))\n",
    "            samples_theta = dist_theta.sample(torch.Size((batch_size,)))\n",
    "\n",
    "        actions = (\n",
    "            torch.stack(\n",
    "                [\n",
    "                    samples_r * torch.cos(torch.pi / 2.0 * samples_theta),\n",
    "                    samples_r * torch.sin(torch.pi / 2.0 * samples_theta),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            * env.delta\n",
    "        )\n",
    "        logprobs = (\n",
    "            dist_r.log_prob(samples_r)\n",
    "            + dist_theta.log_prob(samples_theta)\n",
    "            - torch.log(samples_r * env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - np.log(env.delta)  # why ?\n",
    "        )\n",
    "        return actions, logprobs, samples_r, samples_theta\n",
    "    else:\n",
    "        dist = out\n",
    "\n",
    "        # Automatic termination: check if min_i(1 - state_i) <= env.delta\n",
    "        # This means at least one dimension is within delta of the boundary\n",
    "        should_terminate = torch.any(states >= 1 - env.delta, dim=-1)\n",
    "\n",
    "        A = torch.where(\n",
    "            states[:, 0] <= 1 - env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((1 - states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            states[:, 1] <= 1 - env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((1 - states[:, 1]) / env.delta),\n",
    "        )\n",
    "        assert torch.all(\n",
    "            B[~should_terminate] >= A[~should_terminate]\n",
    "        )\n",
    "        if model.uniform_ratio:\n",
    "            samples = torch.rand(batch_size, device=env.device)\n",
    "        else:\n",
    "            samples = dist.sample()\n",
    "\n",
    "        actions = samples * (B - A) + A\n",
    "        actions *= torch.pi / 2.0\n",
    "        actions = (\n",
    "            torch.stack([torch.cos(actions), torch.sin(actions)], dim=1) * env.delta\n",
    "        )\n",
    "\n",
    "        logprobs = (\n",
    "            dist.log_prob(samples)\n",
    "            - np.log(env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - torch.log(B - A)\n",
    "        )\n",
    "\n",
    "        # Set terminal actions and zero logprobs for terminated states\n",
    "        actions[should_terminate] = -float(\"inf\")\n",
    "        logprobs[should_terminate] = 0.0\n",
    "        samples[should_terminate] = -float(\"inf\")\n",
    "    return actions, logprobs, samples\n",
    "\n",
    "\n",
    "def sample_trajectories(env, model, n_trajectories):\n",
    "    step = 0\n",
    "    states = torch.zeros((n_trajectories, env.dim), device=env.device)\n",
    "    actionss = []\n",
    "    sampless = []\n",
    "    trajectories = [states]\n",
    "    trajectories_logprobs = torch.zeros((n_trajectories,), device=env.device)\n",
    "    all_logprobs = []\n",
    "    first = True\n",
    "    while not torch.all(states == env.sink_state):\n",
    "        step_logprobs = torch.full((n_trajectories,), -float(\"inf\"), device=env.device)\n",
    "        non_terminal_mask = torch.all(states != env.sink_state, dim=-1)\n",
    "        actions = torch.full(\n",
    "            (n_trajectories, env.dim), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        samples = torch.full(\n",
    "            (n_trajectories, env.dim), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        if first:\n",
    "            first = False\n",
    "            non_terminal_actions, logprobs, non_terminal_samples_r, non_terminal_samples_theta = sample_actions(\n",
    "                env,\n",
    "                model,\n",
    "                states[non_terminal_mask],\n",
    "            )\n",
    "            non_terminal_samples = torch.cat([non_terminal_samples_r.unsqueeze(-1), non_terminal_samples_theta.unsqueeze(-1)], dim=-1)\n",
    "            samples[non_terminal_mask] = non_terminal_samples.reshape(-1, env.dim)\n",
    "        else:\n",
    "            non_terminal_actions, logprobs, non_terminal_samples = sample_actions(\n",
    "                env,\n",
    "                model,\n",
    "                states[non_terminal_mask],\n",
    "            )\n",
    "            non_terminal_samples = torch.cat([non_terminal_samples.unsqueeze(-1), torch.zeros_like(non_terminal_samples).unsqueeze(-1)], dim=-1)\n",
    "            samples[non_terminal_mask] = non_terminal_samples.reshape(-1, env.dim)\n",
    "        \n",
    "        actions[non_terminal_mask] = non_terminal_actions.reshape(-1, env.dim)\n",
    "        actionss.append(actions)\n",
    "        sampless.append(samples)\n",
    "        states = env.step(states, actions)\n",
    "        trajectories.append(states)\n",
    "        trajectories_logprobs[non_terminal_mask] += logprobs\n",
    "        step_logprobs[non_terminal_mask] = logprobs\n",
    "        all_logprobs.append(step_logprobs)\n",
    "        step += 1\n",
    "    trajectories = torch.stack(trajectories, dim=1)\n",
    "    actionss = torch.stack(actionss, dim=1)\n",
    "    sampless = torch.stack(sampless, dim=1)\n",
    "    all_logprobs = torch.stack(all_logprobs, dim=1)\n",
    "    return trajectories, actionss, trajectories_logprobs, all_logprobs, sampless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ecca82a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, a, l, al, s = sample_trajectories(env, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52c1ddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1494, 5.2854, 3.1336, 2.7636, 2.9630, 2.5759, 3.5160, 3.1856, 3.0356,\n",
       "        5.1557], device='cuda:3', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7bc31bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0084, 0.1035],\n",
       "        [0.0111, 0.0038],\n",
       "        [0.0680, 0.1033],\n",
       "        [0.1169, 0.1327],\n",
       "        [0.1309, 0.0603],\n",
       "        [0.0670, 0.1886],\n",
       "        [0.0757, 0.0160],\n",
       "        [0.1070, 0.0231],\n",
       "        [0.0984, 0.0957],\n",
       "        [0.0084, 0.0113]], device='cuda:3')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f688c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4153, 0.9487],\n",
       "        [0.0468, 0.2099],\n",
       "        [0.4949, 0.6293],\n",
       "        [0.7075, 0.5403],\n",
       "        [0.5764, 0.2750],\n",
       "        [0.8008, 0.7827],\n",
       "        [0.3097, 0.1327],\n",
       "        [0.4378, 0.1351],\n",
       "        [0.5491, 0.4911],\n",
       "        [0.0563, 0.5927]], device='cuda:3')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e5651bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forward_step_logprobs(env, model, current_states, samples):\n",
    "    if torch.all(current_states[0] == 0.0):\n",
    "        dist_r, dist_theta = model.to_dist(current_states)\n",
    "        samples_r = samples[:, 0]\n",
    "        samples_theta = samples[:, 1]\n",
    "\n",
    "        step_logprobs = (\n",
    "            dist_r.log_prob(samples_r)\n",
    "            + dist_theta.log_prob(samples_theta)\n",
    "            - torch.log(samples_r * env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - np.log(env.delta)  # why ?\n",
    "            )\n",
    "        \n",
    "    else:\n",
    "        step_logprobs = torch.zeros((current_states.shape[0],), device=env.device)\n",
    "        should_terminate = torch.any(samples == -float(\"inf\"), dim=-1)\n",
    "        if current_states.shape[0] == should_terminate.sum():\n",
    "            all_terminate = torch.all(samples == -float(\"inf\"), dim=-1)\n",
    "            step_logprobs[all_terminate] = -float(\"inf\")\n",
    "            return step_logprobs\n",
    "        non_terminal_states = current_states[~should_terminate]\n",
    "        non_terminal_samples = samples[~should_terminate]\n",
    "        dist = model.to_dist(non_terminal_states)\n",
    "\n",
    "        A = torch.where(\n",
    "            non_terminal_states[:, 0] <= 1 - env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((1 - non_terminal_states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            non_terminal_states[:, 1] <= 1 - env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((1 - non_terminal_states[:, 1]) / env.delta),\n",
    "        )\n",
    "\n",
    "        non_terminal_step_logprobs = (\n",
    "            dist.log_prob(non_terminal_samples[:,0])\n",
    "            - np.log(env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - torch.log(B - A)\n",
    "        )\n",
    "        step_logprobs[~should_terminate] = non_terminal_step_logprobs\n",
    "        all_terminate = torch.all(samples == -float(\"inf\"), dim=-1)\n",
    "        step_logprobs[all_terminate] = -float(\"inf\")\n",
    "\n",
    "    return step_logprobs\n",
    "\n",
    "def evaluate_forward_logprobs(env, model, trajectories, sampless):\n",
    "    logprobs = torch.zeros((trajectories.shape[0],), device=env.device)\n",
    "    all_logprobs = []\n",
    "    for i in range(trajectories.shape[1] - 1):\n",
    "        current_states = trajectories[:, i]\n",
    "        samples = sampless[:, i]\n",
    "        step_logprobs = evaluate_forward_step_logprobs(env, model, current_states, samples)\n",
    "        is_finite = torch.isfinite(step_logprobs)\n",
    "        logprobs[is_finite] += step_logprobs[is_finite]\n",
    "        all_logprobs.append(step_logprobs)\n",
    "    all_logprobs = torch.stack(all_logprobs, dim=1)\n",
    "    return logprobs, all_logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ee38fb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000],\n",
       "        [0.1169, 0.1327],\n",
       "        [0.1721, 0.3766],\n",
       "        [0.4055, 0.4662],\n",
       "        [0.5486, 0.6712],\n",
       "        [0.7860, 0.7494],\n",
       "        [  -inf,   -inf],\n",
       "        [  -inf,   -inf]], device='cuda:3')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "69f45596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.9741, 9.7854, 6.0274, 6.5951, 6.7972, 6.2694, 7.1664, 7.7247, 6.6555,\n",
       "         8.8568], device='cuda:3', grad_fn=<IndexPutBackward0>),\n",
       " tensor([[3.1494, 0.9376, 0.8984, 0.9887, 0.0000,   -inf,   -inf],\n",
       "         [5.2854, 0.9943, 0.7742, 0.9605, 0.9792, 0.7918, 0.0000],\n",
       "         [3.1336, 0.9710, 0.9796, 0.9432, 0.0000,   -inf,   -inf],\n",
       "         [2.7636, 0.9170, 0.9648, 0.9814, 0.9683, 0.0000,   -inf],\n",
       "         [2.9630, 0.9706, 0.9903, 0.9809, 0.8925, 0.0000,   -inf],\n",
       "         [2.5759, 0.9134, 0.9355, 0.9877, 0.8569, 0.0000,   -inf],\n",
       "         [3.5160, 0.8519, 0.8581, 0.9440, 0.9964, 0.0000,   -inf],\n",
       "         [3.1856, 0.7344, 0.9747, 0.9043, 0.9582, 0.9676, 0.0000],\n",
       "         [3.0356, 0.8622, 0.9374, 0.8929, 0.9274, 0.0000,   -inf],\n",
       "         [5.1557, 0.9358, 0.8403, 0.9957, 0.9292, 0.0000,   -inf]],\n",
       "        device='cuda:3', grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_forward_logprobs(env, model, t, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0a3b1d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3078, 0.7561],\n",
       "        [0.4252, 0.5238],\n",
       "        [0.7631, 0.3748],\n",
       "        [0.5486, 0.6712],\n",
       "        [0.5817, 0.6379],\n",
       "        [0.5750, 0.6237],\n",
       "        [0.6584, 0.3025],\n",
       "        [0.4808, 0.5300],\n",
       "        [0.6701, 0.3931],\n",
       "        [0.7008, 0.2464]], device='cuda:3')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "40a188a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 0.0000],\n",
       "        [0.6205, 0.0000],\n",
       "        [  -inf, 0.0000],\n",
       "        [0.2024, 0.0000],\n",
       "        [0.0626, 0.0000],\n",
       "        [0.9027, 0.0000],\n",
       "        [0.4285, 0.0000],\n",
       "        [0.1791, 0.0000],\n",
       "        [0.1079, 0.0000],\n",
       "        [0.1143, 0.0000]], device='cuda:3')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d443f468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9792, 0.0000, 0.9683, 0.8925, 0.8569, 0.9964, 0.9582, 0.9274,\n",
       "        0.9292], device='cuda:3', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "bb2070cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf],\n",
       "        [-inf, 0.],\n",
       "        [-inf, -inf],\n",
       "        [-inf, -inf],\n",
       "        [-inf, -inf],\n",
       "        [-inf, -inf],\n",
       "        [-inf, -inf],\n",
       "        [-inf, 0.],\n",
       "        [-inf, -inf],\n",
       "        [-inf, -inf]], device='cuda:3')"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[:,6,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4bce2338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-inf, 0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf],\n",
       "       device='cuda:3', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "416750f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-inf, 0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf],\n",
      "       device='cuda:3')\n",
      "tensor([-inf, 0., -inf, -inf, -inf, -inf, -inf, 0., -inf, -inf],\n",
      "       device='cuda:3', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i = 6\n",
    "print(evaluate_forward_step_logprobs(env, model, t[:,i, :], s[:,i,:]))\n",
    "print(al[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "451aeec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 2])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "8ec7628f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 7, 2])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "818d25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class TrajectoryReplayMemory:\n",
    "    \"\"\"\n",
    "    Replay buffer for trajectories with variable lengths.\n",
    "    \n",
    "    Stores:\n",
    "    - trajectories: shape (capacity, max_traj_len, 2)\n",
    "    - samples: shape (capacity, max_traj_len - 1, 2)\n",
    "    \n",
    "    Uses lazy initialization like SAC replay memory - buffers are allocated on first push.\n",
    "    Automatically handles varying max_traj_len across batches by tracking and updating.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, seed, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: Maximum number of trajectories to store\n",
    "            seed: Random seed for reproducibility\n",
    "            device: Device to store tensors on\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        # Will be initialized on first push\n",
    "        self.trajectories = None\n",
    "        self.samples = None\n",
    "        self.max_traj_len = 0\n",
    "        \n",
    "    def push_batch(self, trajectories, samples):\n",
    "        \"\"\"\n",
    "        Push a batch of trajectories to the replay buffer.\n",
    "        \n",
    "        Args:\n",
    "            trajectories: tensor of shape (batch_size, traj_len, 2)\n",
    "            samples: tensor of shape (batch_size, traj_len - 1, 2)\n",
    "        \"\"\"\n",
    "        batch_size = trajectories.shape[0]\n",
    "        traj_len = trajectories.shape[1]\n",
    "        \n",
    "        # Move to device if needed\n",
    "        trajectories = trajectories.to(self.device)\n",
    "        samples = samples.to(self.device)\n",
    "        \n",
    "        # Initialize or resize buffers if needed\n",
    "        if self.trajectories is None:\n",
    "            # First push - initialize buffers\n",
    "            self.max_traj_len = traj_len\n",
    "            self.trajectories = torch.full(\n",
    "                (self.capacity, traj_len, 2), \n",
    "                -float('inf'), \n",
    "                dtype=trajectories.dtype, \n",
    "                device=self.device\n",
    "            )\n",
    "            self.samples = torch.full(\n",
    "                (self.capacity, traj_len - 1, 2), \n",
    "                -float('inf'), \n",
    "                dtype=samples.dtype, \n",
    "                device=self.device\n",
    "            )\n",
    "        elif traj_len > self.max_traj_len:\n",
    "            # Need to resize buffers to accommodate longer trajectories\n",
    "            old_max_len = self.max_traj_len\n",
    "            self.max_traj_len = traj_len\n",
    "            \n",
    "            # Create new larger buffers\n",
    "            new_trajectories = torch.full(\n",
    "                (self.capacity, traj_len, 2), \n",
    "                -float('inf'), \n",
    "                dtype=self.trajectories.dtype, \n",
    "                device=self.device\n",
    "            )\n",
    "            new_samples = torch.full(\n",
    "                (self.capacity, traj_len - 1, 2), \n",
    "                -float('inf'), \n",
    "                dtype=self.samples.dtype, \n",
    "                device=self.device\n",
    "            )\n",
    "            \n",
    "            # Copy old data to new buffers\n",
    "            new_trajectories[:, :old_max_len, :] = self.trajectories\n",
    "            new_samples[:, :old_max_len - 1, :] = self.samples\n",
    "            \n",
    "            self.trajectories = new_trajectories\n",
    "            self.samples = new_samples\n",
    "        \n",
    "        # Calculate indices\n",
    "        end_pos = self.position + batch_size\n",
    "        \n",
    "        if end_pos <= self.capacity:\n",
    "            # No wrap around\n",
    "            # Reset to -inf first (to handle varying lengths)\n",
    "            self.trajectories[self.position:end_pos] = -float('inf')\n",
    "            self.samples[self.position:end_pos] = -float('inf')\n",
    "            \n",
    "            # Write actual data\n",
    "            self.trajectories[self.position:end_pos, :traj_len, :] = trajectories\n",
    "            self.samples[self.position:end_pos, :traj_len - 1, :] = samples\n",
    "        else:\n",
    "            # Wrap around\n",
    "            first_part = self.capacity - self.position\n",
    "            \n",
    "            # First part\n",
    "            self.trajectories[self.position:] = -float('inf')\n",
    "            self.samples[self.position:] = -float('inf')\n",
    "            self.trajectories[self.position:, :traj_len, :] = trajectories[:first_part]\n",
    "            self.samples[self.position:, :traj_len - 1, :] = samples[:first_part]\n",
    "            \n",
    "            # Second part\n",
    "            second_part = batch_size - first_part\n",
    "            self.trajectories[:second_part] = -float('inf')\n",
    "            self.samples[:second_part] = -float('inf')\n",
    "            self.trajectories[:second_part, :traj_len, :] = trajectories[first_part:]\n",
    "            self.samples[:second_part, :traj_len - 1, :] = samples[first_part:]\n",
    "        \n",
    "        self.position = end_pos % self.capacity\n",
    "        self.size = min(self.size + batch_size, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a random batch from the buffer.\n",
    "        \n",
    "        Returns:\n",
    "            trajectories: tensor of shape (batch_size, max_traj_len, 2)\n",
    "            samples: tensor of shape (batch_size, max_traj_len - 1, 2)\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        indices = torch.from_numpy(indices).to(self.device)\n",
    "        \n",
    "        return (\n",
    "            self.trajectories[indices],\n",
    "            self.samples[indices],\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ba86f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem=TrajectoryReplayMemory(5, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "2d91c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.push_batch(t,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4cc3a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_actions(env, model, states):\n",
    "    # states is a tensor of shape (n, dim)\n",
    "    batch_size = states.shape[0]\n",
    "    out = model.to_dist(states)\n",
    "    if isinstance(out, tuple):  # s0 input returns (dist_r, dist_theta)\n",
    "        dist_r, dist_theta = out\n",
    "        if model.uniform_ratio:\n",
    "            samples_r = torch.rand(batch_size, device=env.device)\n",
    "            samples_theta = torch.rand(batch_size, device=env.device)\n",
    "        else:\n",
    "            samples_r = dist_r.sample(torch.Size((batch_size,)))\n",
    "            samples_theta = dist_theta.sample(torch.Size((batch_size,)))\n",
    "\n",
    "        actions = (\n",
    "            torch.stack(\n",
    "                [\n",
    "                    samples_r * torch.cos(torch.pi / 2.0 * samples_theta),\n",
    "                    samples_r * torch.sin(torch.pi / 2.0 * samples_theta),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            * env.delta\n",
    "        )\n",
    "\n",
    "        return actions, samples_r, samples_theta\n",
    "    else:\n",
    "        dist = out\n",
    "\n",
    "        # Automatic termination: check if min_i(1 - state_i) <= env.delta\n",
    "        # This means at least one dimension is within delta of the boundary\n",
    "        should_terminate = torch.any(states >= 1 - env.delta, dim=-1)\n",
    "\n",
    "        A = torch.where(\n",
    "            states[:, 0] <= 1 - env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((1 - states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            states[:, 1] <= 1 - env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((1 - states[:, 1]) / env.delta),\n",
    "        )\n",
    "        assert torch.all(\n",
    "            B[~should_terminate] >= A[~should_terminate]\n",
    "        )\n",
    "        if model.uniform_ratio:\n",
    "            samples = torch.rand(batch_size, device=env.device)\n",
    "        else:\n",
    "            samples = dist.sample()\n",
    "\n",
    "        actions = samples * (B - A) + A\n",
    "        actions *= torch.pi / 2.0\n",
    "        actions = (\n",
    "            torch.stack([torch.cos(actions), torch.sin(actions)], dim=1) * env.delta\n",
    "        )\n",
    "\n",
    "        # Set terminal actions and zero logprobs for terminated states\n",
    "        actions[should_terminate] = -float(\"inf\")\n",
    "        samples[should_terminate] = -float(\"inf\")\n",
    "    return actions, samples\n",
    "\n",
    "\n",
    "def sample_trajectories(env, model, n_trajectories):\n",
    "    states = torch.zeros((n_trajectories, env.dim), device=env.device)\n",
    "    actionss = []\n",
    "    sampless = []\n",
    "    trajectories = [states]\n",
    "    first = True\n",
    "    while not torch.all(states == env.sink_state):\n",
    "        non_terminal_mask = torch.all(states != env.sink_state, dim=-1)\n",
    "        actions = torch.full(\n",
    "            (n_trajectories, env.dim), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        samples = torch.full(\n",
    "            (n_trajectories, env.dim), -float(\"inf\"), device=env.device\n",
    "        )\n",
    "        if first:\n",
    "            first = False\n",
    "            non_terminal_actions, non_terminal_samples_r, non_terminal_samples_theta = sample_actions(\n",
    "                env,\n",
    "                model,\n",
    "                states[non_terminal_mask],\n",
    "            )\n",
    "            non_terminal_samples = torch.cat([non_terminal_samples_r.unsqueeze(-1), non_terminal_samples_theta.unsqueeze(-1)], dim=-1)\n",
    "            samples[non_terminal_mask] = non_terminal_samples.reshape(-1, env.dim)\n",
    "        else:\n",
    "            non_terminal_actions, non_terminal_samples = sample_actions(\n",
    "                env,\n",
    "                model,\n",
    "                states[non_terminal_mask],\n",
    "            )\n",
    "            non_terminal_samples = torch.cat([non_terminal_samples.unsqueeze(-1), torch.zeros_like(non_terminal_samples).unsqueeze(-1)], dim=-1)\n",
    "            samples[non_terminal_mask] = non_terminal_samples.reshape(-1, env.dim)\n",
    "        \n",
    "        actions[non_terminal_mask] = non_terminal_actions.reshape(-1, env.dim)\n",
    "        actionss.append(actions)\n",
    "        sampless.append(samples)\n",
    "        states = env.step(states, actions)\n",
    "        trajectories.append(states)\n",
    "    trajectories = torch.stack(trajectories, dim=1)\n",
    "    actionss = torch.stack(actionss, dim=1)\n",
    "    sampless = torch.stack(sampless, dim=1)\n",
    "    return trajectories, actionss, sampless\n",
    "\n",
    "\n",
    "def evaluate_forward_step_logprobs(env, model, current_states, samples):\n",
    "    if torch.all(current_states[0] == 0.0):\n",
    "        dist_r, dist_theta = model.to_dist(current_states)\n",
    "        samples_r = samples[:, 0]\n",
    "        samples_theta = samples[:, 1]\n",
    "\n",
    "        step_logprobs = (\n",
    "            dist_r.log_prob(samples_r)\n",
    "            + dist_theta.log_prob(samples_theta)\n",
    "            - torch.log(samples_r * env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - np.log(env.delta)  # why ?\n",
    "            )\n",
    "        \n",
    "    else:\n",
    "        step_logprobs = torch.zeros((current_states.shape[0],), device=env.device)\n",
    "        should_terminate = torch.any(samples == -float(\"inf\"), dim=-1)\n",
    "        if current_states.shape[0] == should_terminate.sum():\n",
    "            all_terminate = torch.all(samples == -float(\"inf\"), dim=-1)\n",
    "            step_logprobs[all_terminate] = -float(\"inf\")\n",
    "            return step_logprobs\n",
    "        non_terminal_states = current_states[~should_terminate]\n",
    "        non_terminal_samples = samples[~should_terminate]\n",
    "        dist = model.to_dist(non_terminal_states)\n",
    "\n",
    "        A = torch.where(\n",
    "            non_terminal_states[:, 0] <= 1 - env.delta,\n",
    "            0.0,\n",
    "            2.0 / torch.pi * torch.arccos((1 - non_terminal_states[:, 0]) / env.delta),\n",
    "        )\n",
    "        B = torch.where(\n",
    "            non_terminal_states[:, 1] <= 1 - env.delta,\n",
    "            1.0,\n",
    "            2.0 / torch.pi * torch.arcsin((1 - non_terminal_states[:, 1]) / env.delta),\n",
    "        )\n",
    "\n",
    "        non_terminal_step_logprobs = (\n",
    "            dist.log_prob(non_terminal_samples[:,0])\n",
    "            - np.log(env.delta)\n",
    "            - np.log(np.pi / 2)\n",
    "            - torch.log(B - A)\n",
    "        )\n",
    "        step_logprobs[~should_terminate] = non_terminal_step_logprobs\n",
    "        all_terminate = torch.all(samples == -float(\"inf\"), dim=-1)\n",
    "        step_logprobs[all_terminate] = -float(\"inf\")\n",
    "\n",
    "    return step_logprobs\n",
    "\n",
    "def evaluate_forward_logprobs(env, model, trajectories, sampless):\n",
    "    logprobs = torch.zeros((trajectories.shape[0],), device=env.device)\n",
    "    all_logprobs = []\n",
    "    for i in range(trajectories.shape[1] - 1):\n",
    "        current_states = trajectories[:, i]\n",
    "        samples = sampless[:, i]\n",
    "        step_logprobs = evaluate_forward_step_logprobs(env, model, current_states, samples)\n",
    "        is_finite = torch.isfinite(step_logprobs)\n",
    "        logprobs[is_finite] += step_logprobs[is_finite]\n",
    "        all_logprobs.append(step_logprobs)\n",
    "    all_logprobs = torch.stack(all_logprobs, dim=1)\n",
    "\n",
    "    return logprobs, all_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca350412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000],\n",
       "          [0.1070, 0.0231],\n",
       "          [0.1155, 0.2729],\n",
       "          [0.2335, 0.4933],\n",
       "          [0.4808, 0.5300],\n",
       "          [0.7210, 0.5994],\n",
       "          [0.9604, 0.6714],\n",
       "          [  -inf,   -inf]]], device='cuda:3'),\n",
       " tensor([[[0.4378, 0.1351],\n",
       "          [0.9784, 0.0000],\n",
       "          [0.6869, 0.0000],\n",
       "          [0.0938, 0.0000],\n",
       "          [0.1791, 0.0000],\n",
       "          [0.1860, 0.0000],\n",
       "          [  -inf, 0.0000]]], device='cuda:3'))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "7d11764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, a, s= sample_trajectories(env, model, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b014a2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6.3049,  8.0581, 10.6425,  5.4570,  6.0205,  6.9582, 11.1457,  8.3006,\n",
       "          9.4937,  6.6070,  6.1785,  9.1929,  6.5551,  5.3420,  8.7167,  9.6875,\n",
       "          5.5324,  8.1981, 10.0643,  5.1203,  6.6051,  6.2696,  6.8125,  7.5544,\n",
       "          8.0624,  7.0409,  7.3496,  5.7086,  8.5025,  7.6846,  6.8814,  8.3864,\n",
       "          6.3313,  7.9154,  5.5941,  5.5203,  9.9390,  6.5189,  8.5146,  7.2337,\n",
       "          7.2535,  5.3556,  6.8767,  6.9977,  7.7717,  6.6833,  6.8756,  5.8708,\n",
       "          9.7061,  5.9830,  6.7616,  7.4976,  6.8159,  5.6410,  6.7580,  8.9903,\n",
       "          6.8381,  5.7219,  5.6903,  9.8904,  7.5038,  8.8829,  5.9186,  8.1108,\n",
       "          4.9215,  6.3091,  8.6169,  7.2356,  6.9763,  6.4249,  7.0409,  6.4821,\n",
       "          6.8562,  5.6809,  8.6254,  6.5991,  5.2656,  6.2546,  7.0892,  8.0111,\n",
       "          8.0538,  5.9393,  8.9342,  9.1009,  7.3086,  6.0919,  5.0975,  5.3005,\n",
       "          6.8856,  6.6196,  7.9170,  7.4525,  6.5360,  8.4903,  6.7787,  8.2739,\n",
       "          5.9961,  8.2345,  6.9771,  8.0467,  6.7223,  6.8066, 10.0759,  5.9995,\n",
       "          6.8724,  6.0004,  7.0773,  6.8339,  7.6069,  6.1630,  7.4603,  6.6491,\n",
       "          6.0134,  5.7110,  6.7763,  5.2897,  6.3241,  6.0573,  7.7819,  6.6489,\n",
       "          5.0789,  7.2016,  7.0360,  5.3254,  7.6884,  6.5327,  5.3423,  9.1687,\n",
       "          9.3075,  5.5870,  5.0862,  8.0907,  6.1054,  5.7285,  6.1683,  6.8689,\n",
       "          5.2111,  6.6368,  5.6505,  6.7580,  7.2673,  6.2112,  8.7129,  6.2988,\n",
       "          7.2291,  6.1496,  6.6507,  5.1165,  7.2583,  6.5818,  7.1473,  7.5902,\n",
       "          6.3437,  8.5220,  7.4402,  7.1756,  5.5406,  6.5051,  9.4716,  5.3234,\n",
       "          7.1033,  9.5797,  6.6405,  6.8555,  6.2458,  5.4041,  7.6880,  9.2077,\n",
       "          5.8818,  6.5110,  8.3241,  5.0482,  5.1442,  6.2025,  9.2862,  6.5334,\n",
       "          7.7264,  5.6716,  7.7499,  6.2270,  7.1286,  6.8919,  7.3340,  7.5504,\n",
       "          7.0086,  9.9280,  8.2231,  8.1295,  8.0215,  5.8638,  7.2556,  7.2222,\n",
       "         10.0977,  8.2655,  8.6153,  5.2018,  9.8841,  6.9218,  7.0327,  5.9531,\n",
       "          6.9603,  5.6961,  7.8567,  8.0438,  6.3322,  6.5750,  6.4659, 11.7377,\n",
       "          9.2276,  8.6276,  8.2046,  6.9034,  7.2404,  6.8526,  9.4598,  6.2244,\n",
       "          5.3467,  6.3072,  5.3451,  6.2195,  6.7103, 10.6720,  6.7061,  8.6280,\n",
       "          5.4246,  6.9155,  7.8182,  8.7594,  6.1010,  6.5899,  7.9684,  7.0320,\n",
       "          8.2632,  6.6520,  6.7237,  5.8003,  7.6402,  6.4860,  7.8118,  7.2388,\n",
       "          5.0720,  5.2185,  6.8985,  7.4406,  5.3652,  7.5606,  5.7186,  7.8812,\n",
       "          9.4179,  5.9847,  5.6173,  6.0648,  6.9752,  7.1684,  5.0097,  7.1400],\n",
       "        device='cuda:3', grad_fn=<IndexPutBackward0>),\n",
       " tensor([[2.5001, 0.8873, 0.9919,  ..., 0.9709, 0.0000,   -inf],\n",
       "         [4.1870, 0.9690, 0.9408,  ..., 0.9844, 0.0000,   -inf],\n",
       "         [6.8166, 0.9444, 0.9311,  ..., 0.9598, 0.0000,   -inf],\n",
       "         ...,\n",
       "         [2.5751, 0.8381, 0.9296,  ..., 0.9688, 0.9209, 0.0000],\n",
       "         [2.3415, 0.9639, 0.9451,  ..., 0.0000,   -inf,   -inf],\n",
       "         [3.4561, 0.8202, 0.9496,  ..., 0.9945, 0.0000,   -inf]],\n",
       "        device='cuda:3', grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_forward_logprobs(env, model, t, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "07495b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.push_batch(t,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974c090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5b5cb855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000],\n",
       "          [0.2091, 0.0612],\n",
       "          [0.4370, 0.1639],\n",
       "          [0.6056, 0.3485],\n",
       "          [0.7357, 0.5620],\n",
       "          [0.9854, 0.5741],\n",
       "          [  -inf,   -inf],\n",
       "          [  -inf,   -inf]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.1055, 0.1287],\n",
       "          [0.3468, 0.1940],\n",
       "          [0.5263, 0.3680],\n",
       "          [0.7670, 0.4353],\n",
       "          [  -inf,   -inf],\n",
       "          [  -inf,   -inf],\n",
       "          [  -inf,   -inf]]], device='cuda:3'),\n",
       " tensor([[[0.8714, 0.1812],\n",
       "          [0.2696, 0.0000],\n",
       "          [0.5289, 0.0000],\n",
       "          [0.6514, 0.0000],\n",
       "          [0.0310, 0.0000],\n",
       "          [  -inf, 0.0000],\n",
       "          [  -inf,   -inf]],\n",
       " \n",
       "         [[0.6655, 0.5629],\n",
       "          [0.1682, 0.0000],\n",
       "          [0.4901, 0.0000],\n",
       "          [0.1736, 0.0000],\n",
       "          [  -inf, 0.0000],\n",
       "          [  -inf,   -inf],\n",
       "          [  -inf,   -inf]]], device='cuda:3'))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tj, sa= mem.sample(2)\n",
    "tj,sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8056b195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7357, 0.5620],\n",
       "        [0.7670, 0.4353]], device='cuda:3')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tj[:,-3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "11988ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:3')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(tj[:,-3,:] == env.sink_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700e35e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "54f5680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "while torch.all(tj[:,-2,:] == env.sink_state):\n",
    "    tj = tj[:,:-1,:]\n",
    "    sa = sa[:,:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ef6156bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000],\n",
       "          [0.2091, 0.0612],\n",
       "          [0.4370, 0.1639],\n",
       "          [0.6056, 0.3485],\n",
       "          [0.7357, 0.5620],\n",
       "          [0.9854, 0.5741],\n",
       "          [  -inf,   -inf]],\n",
       " \n",
       "         [[0.0000, 0.0000],\n",
       "          [0.1055, 0.1287],\n",
       "          [0.3468, 0.1940],\n",
       "          [0.5263, 0.3680],\n",
       "          [0.7670, 0.4353],\n",
       "          [  -inf,   -inf],\n",
       "          [  -inf,   -inf]]], device='cuda:3'),\n",
       " tensor([[[0.8714, 0.1812],\n",
       "          [0.2696, 0.0000],\n",
       "          [0.5289, 0.0000],\n",
       "          [0.6514, 0.0000],\n",
       "          [0.0310, 0.0000],\n",
       "          [  -inf, 0.0000]],\n",
       " \n",
       "         [[0.6655, 0.5629],\n",
       "          [0.1682, 0.0000],\n",
       "          [0.4901, 0.0000],\n",
       "          [0.1736, 0.0000],\n",
       "          [  -inf, 0.0000],\n",
       "          [  -inf,   -inf]]], device='cuda:3'))"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tj, sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497352d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.2270, 5.7186], device='cuda:3', grad_fn=<IndexPutBackward0>),\n",
       " tensor([[2.4423, 0.9734, 0.9939, 0.9756, 0.8418, 0.0000],\n",
       "         [2.8312, 0.9373, 0.9946, 0.9555, 0.0000,   -inf]], device='cuda:3',\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs, all_logprobs= evaluate_forward_logprobs(env, model, tj, sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "dd568506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9854, 0.5741],\n",
       "        [0.7670, 0.4353]], device='cuda:3')"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_states = get_last_states(env, tj)\n",
    "last_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "01d62b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.9078, -6.9078], device='cuda:3')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logrewards = env.reward(last_states).log()\n",
    "logrewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "842fe52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_logprobs, all_bw_logprobs = evaluate_backward_logprobs(\n",
    "    env, bw_model, tj\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a0dec91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.0479, 3.4188], device='cuda:3')"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "bb8870ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.7219, 0.9347, 0.9347, 2.4566],\n",
       "        [0.0000, 1.5050, 0.9347, 0.9791,   -inf]], device='cuda:3')"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bw_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "beb4976a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[294], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[43mlogZ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbw_logprobs\u001b[49m \u001b[38;5;241m-\u001b[39m logrewards) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "loss = torch.mean((logZ + logprobs - bw_logprobs - logrewards) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ce833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
