{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6637d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAC_d0.25_corner_squares_PBuniform_lr0.001_sd425779_gamma0.5_mile2500_batch256\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import argparse\n",
    "\n",
    "from env import Box, get_last_states\n",
    "from sac_model import CirclePB\n",
    "from sac_sampling import (\n",
    "    sample_trajectories,\n",
    "    evaluate_backward_logprobs,\n",
    ")\n",
    "from sac import SAC\n",
    "from sac_replay_memory import ReplayMemory, trajectories_to_transitions\n",
    "\n",
    "from utils import (\n",
    "    fit_kde,\n",
    "    plot_reward,\n",
    "    sample_from_reward,\n",
    "    plot_samples,\n",
    "    estimate_jsd,\n",
    "    plot_trajectories,\n",
    "    plot_termination_probabilities,\n",
    ")\n",
    "\n",
    "import sac_config as config\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--device\", type=str, default=config.DEVICE)\n",
    "parser.add_argument(\"--dim\", type=int, default=config.DIM)\n",
    "parser.add_argument(\"--delta\", type=float, default=config.DELTA)\n",
    "parser.add_argument(\"--R0\", type=float, default=config.R0, help=\"Baseline reward value\")\n",
    "parser.add_argument(\"--R1\", type=float, default=config.R1, help=\"Medium reward value (e.g., outer square)\")\n",
    "parser.add_argument(\"--R2\", type=float, default=config.R2, help=\"High reward value (e.g., inner square)\")\n",
    "parser.add_argument(\"--reward_debug\", action=\"store_true\", default=config.REWARD_DEBUG)\n",
    "parser.add_argument(\n",
    "    \"--reward_type\",\n",
    "    type=str,\n",
    "    choices=[\"baseline\", \"ring\", \"angular_ring\", \"multi_ring\", \"curve\", \"gaussian_mixture\"],\n",
    "    default=config.REWARD_TYPE,\n",
    "    help=\"Type of reward function to use. To modify reward-specific parameters (radius, sigma, etc.), edit rewards.py\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta_min\",\n",
    "    type=float,\n",
    "    default=config.BETA_MIN,\n",
    "    help=\"Minimum value for the concentration parameters of the Beta distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta_max\",\n",
    "    type=float, \n",
    "    default=config.BETA_MAX,\n",
    "    help=\"Maximum value for the concentration parameters of the Beta distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--PB\",\n",
    "    type=str,\n",
    "    choices=[\"learnable\", \"tied\", \"uniform\"],\n",
    "    default=config.PB,\n",
    "    help=\"Backward policy type\",\n",
    ")\n",
    "parser.add_argument(\"--gamma_scheduler\", type=float, default=config.GAMMA_SCHEDULER)\n",
    "parser.add_argument(\"--scheduler_milestone\", type=int, default=config.SCHEDULER_MILESTONE)\n",
    "parser.add_argument(\"--seed\", type=int, default=config.SEED)\n",
    "parser.add_argument(\"--lr\", type=float, default=config.LR, help=\"Learning rate for SAC\")\n",
    "parser.add_argument(\"--BS\", type=int, default=config.BS)\n",
    "parser.add_argument(\"--n_iterations\", type=int, default=config.N_ITERATIONS)\n",
    "parser.add_argument(\"--hidden_dim\", type=int, default=config.HIDDEN_DIM)\n",
    "parser.add_argument(\"--n_hidden\", type=int, default=config.N_HIDDEN)\n",
    "parser.add_argument(\"--n_evaluation_trajectories\", type=int, default=config.N_EVALUATION_TRAJECTORIES)\n",
    "parser.add_argument(\"--no_plot\", action=\"store_true\", default=config.NO_PLOT)\n",
    "parser.add_argument(\"--no_wandb\", action=\"store_true\", default=config.NO_WANDB)\n",
    "parser.add_argument(\"--wandb_project\", type=str, default=config.WANDB_PROJECT)\n",
    "\n",
    "# SAC-specific arguments\n",
    "parser.add_argument(\"--tau\", type=float, default=config.TAU, help=\"Tau for soft update\")\n",
    "parser.add_argument(\"--target_update_interval\", type=int, default=config.TARGET_UPDATE_INTERVAL, help=\"Target network update interval\")\n",
    "parser.add_argument(\"--Critic_hidden_size\", type=int, default=config.CRITIC_HIDDEN_SIZE, help=\"Hidden size for SAC critic networks\")\n",
    "parser.add_argument(\"--replay_size\", type=int, default=config.REPLAY_SIZE, help=\"Replay buffer size\")\n",
    "parser.add_argument(\"--sac_batch_size\", type=int, default=config.SAC_BATCH_SIZE, help=\"SAC batch size\")\n",
    "parser.add_argument(\"--updates_per_step\", type=int, default=config.UPDATES_PER_STEP, help=\"SAC updates per step\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "device = args.device\n",
    "dim = args.dim\n",
    "delta = args.delta\n",
    "seed = args.seed\n",
    "lr = args.lr\n",
    "n_iterations = args.n_iterations\n",
    "BS = args.BS\n",
    "\n",
    "if seed == 0:\n",
    "    seed = np.random.randint(int(1e6))\n",
    "\n",
    "run_name = f\"SAC_d{delta}_{args.reward_type}_PB{args.PB}_lr{lr}_sd{seed}\"\n",
    "run_name += f\"_gamma{args.gamma_scheduler}_mile{args.scheduler_milestone}\"\n",
    "run_name += f\"_batch{args.sac_batch_size}\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcea81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sac_utils import soft_update, hard_update\n",
    "from sac_model import QNetwork\n",
    "from sac_model import CirclePF\n",
    "from sac_sampling import sample_actions\n",
    "\n",
    "\n",
    "class SAC(object):\n",
    "    def __init__(self, args, env):\n",
    "\n",
    "        self.gamma = 1.0  # Fixed to 1.0 for GFlowNet\n",
    "        self.target_update_interval = args.target_update_interval\n",
    "        self.device = env.device\n",
    "        self.env = env  # Store env for sample_actions\n",
    "        self.critic = QNetwork(env.dim, env.dim, args.Critic_hidden_size).to(device=self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=args.lr)\n",
    "        self.critic_target = QNetwork(env.dim, env.dim, args.Critic_hidden_size).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "        self.policy = CirclePF(\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            n_hidden=args.n_hidden,\n",
    "            beta_min=args.beta_min,\n",
    "            beta_max=args.beta_max,\n",
    "        ).to(self.device)\n",
    "        self.policy_optim = Adam(self.policy.parameters(), lr=args.lr)\n",
    "        # Add scheduler for policy optimizer\n",
    "        self.policy_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            self.policy_optim,\n",
    "            milestones=[i * args.scheduler_milestone for i in range(1, 10)],\n",
    "            gamma=args.gamma_scheduler,\n",
    "        )\n",
    "\n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        # Sample a batch from memory\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(batch_size=batch_size)\n",
    "        print(state_batch.shape)\n",
    "        print(action_batch.shape)\n",
    "        print(reward_batch.shape)\n",
    "        print(next_state_batch.shape)\n",
    "        print(done_batch.shape)\n",
    "        # â˜… ì—¬ê¸°ì—ì„œ ê·¸ëž˜í”„ë¥¼ í™•ì‹¤ížˆ ëŠì–´ì£¼ê¸°\n",
    "        state_batch      = state_batch.detach().to(self.device)\n",
    "        action_batch     = action_batch.detach().to(self.device)\n",
    "        reward_batch     = reward_batch.detach().to(self.device).unsqueeze(-1)   # (B, 1)ë¡œ\n",
    "        next_state_batch = next_state_batch.detach().to(self.device)\n",
    "        done_batch       = done_batch.detach().to(self.device).unsqueeze(-1)     # (B, 1)ë¡œ\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Problem 1: Handle sink states in next_state_batch\n",
    "            # Identify non-sink next states\n",
    "            non_sink_mask = ~torch.all(next_state_batch == self.env.sink_state, dim=-1)\n",
    "\n",
    "            # Initialize with zeros (will be masked out anyway)\n",
    "            next_state_action = torch.zeros_like(next_state_batch)\n",
    "            next_state_log_pi = torch.zeros(next_state_batch.shape[0], device=self.device)\n",
    "\n",
    "            # Only sample actions for non-sink next states\n",
    "            if non_sink_mask.any():\n",
    "                sampled_actions, sampled_log_pi = sample_actions(\n",
    "                    self.env, self.policy, next_state_batch[non_sink_mask]\n",
    "                )\n",
    "                # Replace -inf terminal actions with zeros (won't be used anyway due to done_batch)\n",
    "                sampled_actions = torch.where(  \n",
    "                    torch.isinf(sampled_actions),\n",
    "                    torch.zeros_like(sampled_actions),\n",
    "                    sampled_actions\n",
    "                )\n",
    "                next_state_action[non_sink_mask] = sampled_actions\n",
    "                next_state_log_pi[non_sink_mask] = sampled_log_pi\n",
    "\n",
    "            # Replace sink states with zeros for critic input (won't be used due to done_batch)\n",
    "            next_state_batch_clean = torch.where(\n",
    "                torch.isinf(next_state_batch),\n",
    "                torch.zeros_like(next_state_batch),\n",
    "                next_state_batch\n",
    "            )\n",
    "\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch_clean, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - next_state_log_pi.unsqueeze(-1)\n",
    "            # FIXED: done_batch is \"done\" (1=done, 0=not done)\n",
    "            # Correct SAC Bellman: Q(s,a) = r + (1-done) * Î³ * Q(s',a')\n",
    "            # Intermediate (done=0): Q = r + 1*Î³*Q' = r + Î³*Q' âœ“\n",
    "            # Terminal (done=1): Q = r + 0*Î³*Q' = r âœ“\n",
    "            next_q_value = reward_batch + (1 - done_batch) * (min_qf_next_target)\n",
    "\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = ð”¼(st,at)~D[0.5(Q1(st,at) - r(st,at) - Î³(ð”¼st+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        s0_mask = torch.all(state_batch == 0, dim=-1)\n",
    "        non_s0_mask = ~s0_mask\n",
    "        if s0_mask.any():\n",
    "            pi_s0, log_pi_s0 = sample_actions(self.env, self.policy, state_batch[s0_mask])\n",
    "\n",
    "        if non_s0_mask.any():\n",
    "            pi_non_s0, log_pi_non_s0 = sample_actions(self.env, self.policy, state_batch[non_s0_mask])\n",
    "            \n",
    "        pi = torch.cat([pi_s0, pi_non_s0], dim=0)\n",
    "        log_pi = torch.cat([log_pi_s0, log_pi_non_s0], dim=0)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = (log_pi - min_qf_pi).mean() # JÏ€ = ð”¼stâˆ¼D,Îµtâˆ¼N[Î± * logÏ€(f(Îµt;st)|st) âˆ’ Q(st,f(Îµt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic)\n",
    "\n",
    "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item()\n",
    "        \n",
    "    # Save model parameters\n",
    "    def save_checkpoint(self, env_name, suffix=\"\", ckpt_path=None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "        if ckpt_path is None:\n",
    "            ckpt_path = \"checkpoints/sac_checkpoint_{}_{}\".format(env_name, suffix)\n",
    "        print('Saving models to {}'.format(ckpt_path))\n",
    "        torch.save({'policy_state_dict': self.policy.state_dict(),\n",
    "                    'critic_state_dict': self.critic.state_dict(),\n",
    "                    'critic_target_state_dict': self.critic_target.state_dict(),\n",
    "                    'critic_optimizer_state_dict': self.critic_optim.state_dict(),\n",
    "                    'policy_optimizer_state_dict': self.policy_optim.state_dict()}, ckpt_path)\n",
    "\n",
    "    # Load model parameters\n",
    "    def load_checkpoint(self, ckpt_path, evaluate=False):\n",
    "        print('Loading models from {}'.format(ckpt_path))\n",
    "        if ckpt_path is not None:\n",
    "            checkpoint = torch.load(ckpt_path)\n",
    "            self.policy.load_state_dict(checkpoint['policy_state_dict'])\n",
    "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
    "            self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])\n",
    "            self.critic_optim.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
    "            self.policy_optim.load_state_dict(checkpoint['policy_optimizer_state_dict'])\n",
    "\n",
    "            if evaluate:\n",
    "                self.policy.eval()\n",
    "                self.critic.eval()\n",
    "                self.critic_target.eval()\n",
    "            else:\n",
    "                self.policy.train()\n",
    "                self.critic.train()\n",
    "                self.critic_target.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "env = Box(\n",
    "    dim=dim,\n",
    "    delta=delta,\n",
    "    device_str=device,\n",
    "    reward_type=args.reward_type,\n",
    "    reward_debug=args.reward_debug,\n",
    "    R0=args.R0,\n",
    "    R1=args.R1,\n",
    "    R2=args.R2,\n",
    ")\n",
    "\n",
    "# Get the true KDE\n",
    "samples = sample_from_reward(env, n_samples=10000)\n",
    "true_kde, fig1 = fit_kde(samples, plot=True)\n",
    "\n",
    "# Create SAC agent (includes CirclePF as policy)\n",
    "sac_agent = SAC(args, env)\n",
    "\n",
    "# Create replay memory\n",
    "memory = ReplayMemory(args.replay_size, seed, device=device)\n",
    "\n",
    "bw_model = CirclePB(\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    n_hidden=args.n_hidden,\n",
    "    torso=sac_agent.policy.torso if args.PB == \"tied\" else None,\n",
    "    uniform=args.PB == \"uniform\",\n",
    "    beta_min=args.beta_min,\n",
    "    beta_max=args.beta_max,\n",
    ").to(device)\n",
    "\n",
    "jsd = float(\"inf\")\n",
    "sac_updates = 0  # Track SAC update steps\n",
    "\n",
    "for i in trange(1, n_iterations + 1):\n",
    "    print(f\"Iteration {i}\")\n",
    "\n",
    "    trajectories, actionss, logprobs, all_logprobs = sample_trajectories(\n",
    "        env,\n",
    "        sac_agent.policy,\n",
    "        BS,\n",
    "    )\n",
    "    last_states = get_last_states(env, trajectories)\n",
    "    logrewards = env.reward(last_states).log()\n",
    "    bw_logprobs, all_bw_logprobs = evaluate_backward_logprobs(\n",
    "        env, bw_model, trajectories\n",
    "    )\n",
    "\n",
    "    # Convert trajectories to transitions and push to replay memory\n",
    "    all_states, all_actions, all_rewards, all_next_states, all_dones = trajectories_to_transitions(\n",
    "        trajectories, actionss, all_bw_logprobs, logrewards, env\n",
    "    )\n",
    "    memory.push_batch(all_states, all_actions, all_rewards, all_next_states, all_dones)\n",
    "\n",
    "    \n",
    "    # TB (Trajectory Balance) loss\n",
    "    if len(memory) > args.sac_batch_size:\n",
    "        for _ in range(args.updates_per_step):\n",
    "            qf1_loss, qf2_loss, policy_loss = sac_agent.update_parameters(memory, args.sac_batch_size, sac_updates)\n",
    "            sac_updates += 1\n",
    "        # Step the scheduler once per iteration (not per update)\n",
    "        sac_agent.policy_scheduler.step()\n",
    "    break\n",
    "    if any(\n",
    "        [\n",
    "            torch.isnan(list(sac_agent.policy.parameters())[i]).any()\n",
    "            for i in range(len(list(sac_agent.policy.parameters())))\n",
    "        ]\n",
    "    ):\n",
    "        raise ValueError(\"NaN in model parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CGFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
